{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Development Preview</p> <p>This is a preview of the documentation from PR #45.</p> <p>For the stable documentation, visit nouuu.github.io/voice-transcriber</p>"},{"location":"#voice-transcriber-documentation","title":"Voice Transcriber Documentation","text":""},{"location":"#voice-transcriber","title":"\ud83c\udfa4 Voice Transcriber","text":"<p>Lightweight desktop voice-to-text transcription with OpenAI Whisper and system tray integration</p> <p> </p>"},{"location":"#overview","title":"Overview","text":"<p>Voice Transcriber is a lightweight desktop application that provides seamless voice-to-text conversion with system tray integration. Record audio with a single click, and transcribed text is automatically copied to your clipboard.</p> <ul> <li> <p> System Tray Integration</p> <p>Click to record, visual state feedback (green=idle, red=recording, purple=processing)</p> </li> <li> <p> Multilingual Support</p> <p>French, English, Spanish, German, Italian with strong language enforcement</p> </li> <li> <p> AI-Powered</p> <p>OpenAI Whisper transcription with optional GPT text formatting</p> </li> <li> <p> Self-Hosted Option</p> <p>Run 100% offline with Speaches - zero cost, complete privacy</p> </li> </ul>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>\ud83c\udfaf System Tray Integration: Click to record, visual state feedback</li> <li>\ud83c\udf99\ufe0f High-Quality Recording: Audio capture using arecord on Linux</li> <li>\ud83c\udf0d Multilingual Support: French, English, Spanish, German, Italian</li> <li>\u270d\ufe0f Text Formatting: Optional GPT-based grammar improvement</li> <li>\ud83d\udccb Clipboard Integration: Automatic result copying</li> <li>\ud83c\udfe0 Self-Hosted Option: Run 100% offline with Speaches</li> <li>\ud83d\udd12 Privacy-Focused: No persistent audio storage, local processing</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Get started in under 5 minutes:</p> Automated SetupManual Setup <pre><code># Clone and setup\ngit clone https://github.com/Nouuu/voice-transcriber.git\ncd voice-transcriber\nmake setup\n\n# Configure API key\nnano ~/.config/voice-transcriber/config.json\n\n# Run\nmake run\n</code></pre> <pre><code># Install Bun runtime\ncurl -fsSL https://bun.sh/install | bash\n\n# Install system dependencies\nsudo apt-get install alsa-utils xsel\n\n# Clone repository\ngit clone https://github.com/Nouuu/voice-transcriber.git\ncd voice-transcriber\n\n# Install dependencies\nbun install\n\n# Initialize config\nmake init-config\n</code></pre> <p>Next Steps</p> <ul> <li>Installation Guide - Detailed setup instructions</li> <li>Configuration - Configure languages and backends</li> <li>Basic Usage - Learn how to use the app</li> </ul>"},{"location":"#how-it-works","title":"How It Works","text":"<pre><code>sequenceDiagram\n    participant User\n    participant SystemTray\n    participant AudioRecorder\n    participant Whisper as OpenAI Whisper\n    participant GPT as OpenAI GPT\n    participant Clipboard\n\n    User-&gt;&gt;SystemTray: Click tray icon\n    SystemTray-&gt;&gt;SystemTray: State: RECORDING (\ud83d\udd34)\n    SystemTray-&gt;&gt;AudioRecorder: Start recording\n    AudioRecorder-&gt;&gt;AudioRecorder: Capture audio (arecord)\n    User-&gt;&gt;SystemTray: Click again to stop\n    SystemTray-&gt;&gt;SystemTray: State: PROCESSING (\ud83d\udfe3)\n    AudioRecorder-&gt;&gt;AudioRecorder: Save WAV file\n    AudioRecorder-&gt;&gt;Whisper: Upload audio\n    Whisper-&gt;&gt;Whisper: Transcribe audio\n    Whisper--&gt;&gt;AudioRecorder: Return text\n    opt Formatting Enabled\n        AudioRecorder-&gt;&gt;GPT: Format text\n        GPT--&gt;&gt;AudioRecorder: Formatted text\n    end\n    AudioRecorder-&gt;&gt;Clipboard: Copy text\n    Clipboard--&gt;&gt;User: Paste transcription\n    SystemTray-&gt;&gt;SystemTray: State: IDLE (\ud83d\udfe2)</code></pre>"},{"location":"#popular-use-cases","title":"Popular Use Cases","text":"<ul> <li> <p>\ud83d\udcdd Note Taking</p> <p>Record meetings, lectures, or brainstorming sessions with automatic transcription</p> </li> <li> <p>\ud83d\udcac Message Dictation</p> <p>Quickly dictate messages, emails, or social media posts</p> </li> <li> <p>\ud83c\udf10 Language Learning</p> <p>Practice pronunciation and see transcriptions in multiple languages</p> </li> <li> <p>\u267f Accessibility</p> <p>Voice-to-text for users with typing difficulties</p> </li> </ul>"},{"location":"#documentation-structure","title":"Documentation Structure","text":"<ul> <li> <p>Getting Started</p> <p>Installation, configuration, and first-run setup</p> </li> <li> <p>User Guide</p> <p>Basic usage, language support, and troubleshooting</p> </li> <li> <p>Development</p> <p>Architecture, development guide, and API reference</p> </li> <li> <p>Advanced</p> <p>Self-hosted setup, whisper models, and local inference</p> </li> </ul>"},{"location":"#community-and-support","title":"Community and Support","text":"<ul> <li>GitHub Repository: nouuu/voice-transcriber</li> <li>npm Package: voice-transcriber</li> <li>Issues: Report a bug or request a feature</li> <li>Discussions: GitHub Discussions</li> </ul>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License.</p> <p>Built with \u2764\ufe0f using Bun, TypeScript, and OpenAI</p>"},{"location":"MIGRATION_GUIDE/","title":"Documentation Migration Guide","text":""},{"location":"MIGRATION_GUIDE/#summary-of-changes","title":"Summary of Changes","text":"<p>This document tracks the migration from inline README documentation to structured MkDocs documentation.</p>"},{"location":"MIGRATION_GUIDE/#what-was-done","title":"What Was Done","text":"<ol> <li>Created MkDocs Structure (<code>documentation/</code> directory)</li> <li>Getting Started (3 pages)</li> <li>User Guide (4 pages)</li> <li>Development (4 pages)</li> <li>Advanced (3 pages)</li> <li> <p>Contributing (1 page)</p> </li> <li> <p>Migrated Existing Documentation</p> </li> <li>Copied from <code>docs/</code> (gitignored, local-only)</li> <li>Adapted for MkDocs Material theme</li> <li> <p>Added cross-references and navigation</p> </li> <li> <p>Setup CI/CD</p> </li> <li>GitHub Actions workflow (<code>.github/workflows/docs.yml</code>)</li> <li>Automatic deployment to GitHub Pages on <code>main</code> push</li> <li> <p>Build validation on pull requests</p> </li> <li> <p>Added Makefile Commands</p> </li> <li><code>make docs-install</code> - Install MkDocs dependencies</li> <li><code>make docs-build</code> - Build documentation site</li> <li><code>make docs-serve</code> - Preview locally at http://127.0.0.1:8000</li> <li> <p><code>make docs-deploy</code> - Manual deployment (CI handles this)</p> </li> <li> <p>Updated <code>.gitignore</code></p> </li> <li>Added <code>site/</code> (MkDocs build artifacts)</li> <li>Added <code>.cache/</code> (MkDocs cache)</li> <li>Kept <code>docs/</code> (local-only development documentation)</li> </ol>"},{"location":"MIGRATION_GUIDE/#what-needs-to-be-done","title":"What Needs To Be Done","text":""},{"location":"MIGRATION_GUIDE/#1-simplify-main-readmemd","title":"1. Simplify Main README.md","text":"<p>The main README should become a concise landing page that directs users to the comprehensive documentation:</p> <p>Recommended structure:</p> <pre><code># \ud83c\udfa4 Voice Transcriber\n\n[Badges here]\n\nLightweight desktop voice-to-text transcription with OpenAI Whisper and system tray integration.\n\n## \u2728 Features\n\n- \ud83c\udfaf System Tray Integration\n- \ud83c\udf0d Multilingual Support (5 languages)\n- \ud83c\udfe0 Self-Hosted Option (Speaches)\n- \ud83d\udccb Automatic Clipboard Copy\n\n## \ud83d\ude80 Quick Start\n\n```bash\ngit clone https://github.com/Nouuu/voice-transcriber.git\ncd voice-transcriber\nmake setup\nnano ~/.config/voice-transcriber/config.json  # Add API key\nmake run\n</code></pre>"},{"location":"MIGRATION_GUIDE/#documentation","title":"\ud83d\udcda Documentation","text":"<p>Comprehensive documentation available at: https://nouuu.github.io/voice-transcriber</p> <ul> <li>Installation Guide</li> <li>Configuration</li> <li>User Guide</li> <li>Development Guide</li> </ul>"},{"location":"MIGRATION_GUIDE/#contributing","title":"\ud83e\udd1d Contributing","text":"<p>See Contributing Guide for guidelines.</p>"},{"location":"MIGRATION_GUIDE/#license","title":"\ud83d\udcc4 License","text":"<p>MIT License - see LICENSE file. <pre><code>**What to remove from README**:\n\n- \u274c Detailed installation steps (move to docs)\n- \u274c Complete configuration reference (move to docs)\n- \u274c Extensive usage examples (move to docs)\n- \u274c Full troubleshooting guide (move to docs)\n- \u274c Detailed development workflow (move to docs)\n- \u274c Complete roadmap (move to docs)\n\n**What to keep in README**:\n\n- \u2705 Project overview and features\n- \u2705 Quick start command sequence\n- \u2705 Links to comprehensive documentation\n- \u2705 Badges and basic metadata\n- \u2705 License information\n\n#### 2. Enable GitHub Pages\n\nIn your repository settings:\n\n1. Go to **Settings** &gt; **Pages**\n2. Source: **Deploy from a branch**\n3. Branch: **gh-pages** (will be created by CI)\n4. Wait for first deployment from CI\n\n#### 3. Test Documentation Locally\n\n```bash\n# Install dependencies\nmake docs-install\n\n# Preview locally\nmake docs-serve\n</code></pre></p> <p>Open http://127.0.0.1:8000 and verify:</p> <ul> <li>Navigation works correctly</li> <li>All pages load without errors</li> <li>Code examples are properly formatted</li> <li>Internal links resolve correctly</li> <li>Search functionality works</li> </ul>"},{"location":"MIGRATION_GUIDE/#4-deploy-to-github-pages","title":"4. Deploy to GitHub Pages","text":"<p>Option 1: Automatic (Recommended)</p> <p>Push to <code>main</code> branch:</p> <pre><code>git add .\ngit commit -m \"docs: setup MkDocs documentation with CI/CD\"\ngit push origin main\n</code></pre> <p>GitHub Actions will automatically deploy to GitHub Pages.</p> <p>Option 2: Manual</p> <pre><code>make docs-deploy\n</code></pre>"},{"location":"MIGRATION_GUIDE/#5-update-repository-links","title":"5. Update Repository Links","text":"<p>After documentation is live, update:</p> <ul> <li>README.md: Add documentation URL</li> <li>package.json: Update <code>homepage</code> field</li> <li>GitHub repository description: Add documentation link</li> </ul>"},{"location":"MIGRATION_GUIDE/#files-createdmodified","title":"Files Created/Modified","text":""},{"location":"MIGRATION_GUIDE/#created-files","title":"Created Files","text":"<pre><code>documentation/\n\u251c\u2500\u2500 index.md\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 getting-started/\n\u2502   \u251c\u2500\u2500 installation.md\n\u2502   \u251c\u2500\u2500 configuration.md\n\u2502   \u2514\u2500\u2500 first-run.md\n\u251c\u2500\u2500 user-guide/\n\u2502   \u251c\u2500\u2500 basic-usage.md\n\u2502   \u251c\u2500\u2500 language-support.md\n\u2502   \u251c\u2500\u2500 transcription-backends.md\n\u2502   \u2514\u2500\u2500 troubleshooting.md\n\u251c\u2500\u2500 development/\n\u2502   \u251c\u2500\u2500 architecture.md\n\u2502   \u251c\u2500\u2500 development-guide.md\n\u2502   \u251c\u2500\u2500 testing.md\n\u2502   \u2514\u2500\u2500 api-reference.md\n\u251c\u2500\u2500 advanced/\n\u2502   \u251c\u2500\u2500 speaches-integration.md\n\u2502   \u251c\u2500\u2500 whisper-models.md\n\u2502   \u2514\u2500\u2500 local-inference.md\n\u2514\u2500\u2500 contributing.md\n\nmkdocs.yml\n.github/workflows/docs.yml\n</code></pre>"},{"location":"MIGRATION_GUIDE/#modified-files","title":"Modified Files","text":"<pre><code>Makefile (added docs-* commands)\n.gitignore (added site/ and .cache/)\n</code></pre>"},{"location":"MIGRATION_GUIDE/#unchanged-files","title":"Unchanged Files","text":"<pre><code>docs/ (still gitignored, local-only)\nREADME.md (to be simplified by user)\nCLAUDE.md (local-only, no changes needed)\n</code></pre>"},{"location":"MIGRATION_GUIDE/#rollback-plan","title":"Rollback Plan","text":"<p>If needed, documentation migration can be rolled back:</p> <pre><code># Remove MkDocs files\nrm -rf documentation/ site/ mkdocs.yml\n\n# Restore Makefile (remove docs commands)\ngit checkout main -- Makefile\n\n# Restore .gitignore\ngit checkout main -- .gitignore\n\n# Remove GitHub Actions workflow\nrm .github/workflows/docs.yml\n</code></pre>"},{"location":"MIGRATION_GUIDE/#benefits-of-migration","title":"Benefits of Migration","text":""},{"location":"MIGRATION_GUIDE/#before-inline-readme","title":"Before (Inline README)","text":"<ul> <li>\u274c Single 650-line README file</li> <li>\u274c No search functionality</li> <li>\u274c No version history for docs</li> <li>\u274c Difficult to navigate</li> <li>\u274c No syntax highlighting in examples</li> </ul>"},{"location":"MIGRATION_GUIDE/#after-mkdocs","title":"After (MkDocs)","text":"<ul> <li>\u2705 Organized, paginated documentation</li> <li>\u2705 Full-text search across all pages</li> <li>\u2705 Git-tracked version history</li> <li>\u2705 Clear navigation with sections</li> <li>\u2705 Syntax highlighting and admonitions</li> <li>\u2705 Mobile-friendly responsive design</li> <li>\u2705 Automatic deployment with CI/CD</li> </ul>"},{"location":"MIGRATION_GUIDE/#next-steps","title":"Next Steps","text":"<ol> <li>Review all documentation pages for accuracy</li> <li>Simplify main README.md as described above</li> <li>Test local documentation preview with <code>make docs-serve</code></li> <li>Push to main to trigger automatic deployment</li> <li>Verify GitHub Pages deployment at https://nouuu.github.io/voice-transcriber</li> <li>Update repository links to documentation site</li> </ol>"},{"location":"MIGRATION_GUIDE/#questions","title":"Questions?","text":"<ul> <li>Check MkDocs Material Documentation</li> <li>Review Speaches Documentation for inspiration</li> <li>Ask on GitHub Discussions</li> </ul>"},{"location":"contributing/","title":"Contributing Guide","text":"<p>Thank you for considering contributing to Voice Transcriber! This guide will help you get started.</p>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>Be respectful, inclusive, and considerate in all interactions.</p>"},{"location":"contributing/#how-to-contribute","title":"How to Contribute","text":""},{"location":"contributing/#reporting-bugs","title":"Reporting Bugs","text":"<p>Before submitting a bug report:</p> <ol> <li>Check existing issues</li> <li>Try the latest version</li> <li>Enable debug mode: <code>voice-transcriber --debug</code></li> </ol> <p>Bug report should include:</p> <ul> <li>Voice Transcriber version</li> <li>Operating system and desktop environment</li> <li>Steps to reproduce</li> <li>Expected vs actual behavior</li> <li>Debug logs (if applicable)</li> </ul> <p>Submit at: https://github.com/Nouuu/voice-transcriber/issues/new</p>"},{"location":"contributing/#suggesting-features","title":"Suggesting Features","text":"<p>Feature requests should include:</p> <ul> <li>Problem description: What problem does this solve?</li> <li>Proposed solution: How should it work?</li> <li>Alternatives considered: What other approaches did you think about?</li> <li>Use cases: Who benefits from this feature?</li> </ul>"},{"location":"contributing/#submitting-pull-requests","title":"Submitting Pull Requests","text":"<p>Development workflow:</p> <pre><code># 1. Fork and clone\ngit clone https://github.com/YOUR_USERNAME/voice-transcriber.git\ncd voice-transcriber\n\n# 2. Create feature branch\ngit checkout -b feat/your-feature-name\n\n# 3. Setup development environment\nmake setup\n\n# 4. Make changes\n# Edit code...\n\n# 5. Run tests and linting\nmake test\nmake format-check\n\n# 6. Commit changes\ngit commit -m \"feat: add your feature\"\n\n# 7. Push and create PR\ngit push origin feat/your-feature-name\n</code></pre> <p>Pull request checklist:</p> <ul> <li> Code follows project conventions</li> <li> Tests added for new functionality</li> <li> All tests pass (<code>make test</code>)</li> <li> Linting passes (<code>make format-check</code>)</li> <li> Commit messages follow Conventional Commits</li> <li> Documentation updated if needed</li> </ul>"},{"location":"contributing/#development-guidelines","title":"Development Guidelines","text":""},{"location":"contributing/#code-style","title":"Code Style","text":"<ul> <li>TypeScript: Strict typing, no <code>any</code> types</li> <li>Services: 3-5 methods maximum, simple interfaces</li> <li>Error handling: Consistent <code>{ success: boolean, error?: string }</code> pattern</li> <li>Testing: Focus on core functionality, use simple mocks</li> </ul>"},{"location":"contributing/#commit-messages","title":"Commit Messages","text":"<p>Use Conventional Commits format:</p> <pre><code>type: description\n\nfeat: add new feature\nfix: resolve bug in service\nrefactor: simplify code structure\ntest: add tests for component\ndocs: update documentation\nchore: update dependencies\n</code></pre> <p>Rules: - Keep descriptions under 50 characters - Use present tense (\"add\" not \"added\") - No capitalization after colon - No period at end</p>"},{"location":"contributing/#project-principles","title":"Project Principles","text":""},{"location":"contributing/#keep-it-simple","title":"KEEP IT SIMPLE","text":"<p>\u2705 Do: - Basic error handling - Simple configuration - Direct API calls - Console logging (info/error) - Single responsibility</p> <p>\u274c Don't: - Complex retry logic - Advanced statistics - Batch processing (unless essential) - Complex validation - Advanced logging systems</p> <p>Each service should be under 100 lines with 3-5 core methods.</p>"},{"location":"contributing/#testing","title":"Testing","text":"<p>Run tests:</p> <pre><code>make test              # All tests\nmake test-watch        # Watch mode\nmake test-file FILE=   # Specific test\n</code></pre> <p>Testing philosophy:</p> <ul> <li>Test core functionality</li> <li>Use simple mocks</li> <li>Maximum 5-6 tests per service</li> <li>Focus on: success cases, basic errors, input validation</li> </ul>"},{"location":"contributing/#documentation","title":"Documentation","text":""},{"location":"contributing/#updating-documentation","title":"Updating Documentation","text":"<p>Documentation uses MkDocs Material:</p> <pre><code># Install dependencies\nmake docs-install\n\n# Serve locally\nmake docs-serve\n\n# Build\nmake docs-build\n</code></pre> <p>Documentation structure:</p> <pre><code>documentation/\n\u251c\u2500\u2500 getting-started/    # Installation, configuration, first run\n\u251c\u2500\u2500 user-guide/         # Usage, language support, troubleshooting\n\u251c\u2500\u2500 development/        # Architecture, development guide, API\n\u2514\u2500\u2500 advanced/          # Speaches, models, roadmap\n</code></pre>"},{"location":"contributing/#writing-documentation","title":"Writing Documentation","text":"<ul> <li>Clear language: Write for users at different levels</li> <li>Code examples: Include practical examples</li> <li>Cross-references: Link related pages</li> <li>Admonitions: Use tips, warnings, and info boxes</li> </ul> <p>Example:</p> <pre><code>!!! tip \"Recommendation\"\n    Use the base model for best speed/accuracy balance.\n\n!!! warning \"Known Issue\"\n    System tray may not work on all desktop environments.\n</code></pre>"},{"location":"contributing/#community","title":"Community","text":"<ul> <li>GitHub Issues: https://github.com/Nouuu/voice-transcriber/issues</li> <li>GitHub Discussions: https://github.com/Nouuu/voice-transcriber/discussions</li> <li>Pull Requests: https://github.com/Nouuu/voice-transcriber/pulls</li> </ul>"},{"location":"contributing/#recognition","title":"Recognition","text":"<p>All contributors will be recognized in the project's README and release notes.</p> <p>Thank you for contributing to Voice Transcriber! \ud83c\udfa4</p>"},{"location":"advanced/local-inference/","title":"Local Inference Roadmap - Whisper Local Transcription","text":"<p>Goal: Implement local Whisper transcription using CPU-only inference for offline usage and cost reduction.</p> <p>Status: \u2705 Implementation Complete - Documentation Pending Priority: High \ud83d\udd25</p>"},{"location":"advanced/local-inference/#implementation-summary","title":"Implementation Summary","text":"<p>Approach: Speaches (Self-hosted OpenAI-compatible server)</p> <p>Why Speaches: - \u2705 OpenAI API-compatible (drop-in replacement, zero code changes) - \u2705 Docker-based deployment (simple setup) - \u2705 Dynamic model loading (on-demand) - \u2705 Production-ready and actively maintained - \u2705 CPU/GPU support</p> <p>Configuration-Based Routing: <pre><code>{\n  \"transcription\": {\n    \"backend\": \"openai\",  // or \"speaches\"\n    \"openai\": {\n      \"apiKey\": \"sk-...\",\n      \"model\": \"whisper-1\"\n    },\n    \"speaches\": {\n      \"url\": \"http://localhost:8000/v1\",\n      \"apiKey\": \"none\",\n      \"model\": \"Systran/faster-whisper-base\"\n    }\n  }\n}\n</code></pre></p>"},{"location":"advanced/local-inference/#deployment-modes","title":"Deployment Modes","text":""},{"location":"advanced/local-inference/#1-openai-cloud-default","title":"1. OpenAI Cloud (Default) \u2601\ufe0f","text":"<p><pre><code>{ \"transcription\": { \"backend\": \"openai\" } }\n</code></pre> - \u2705 Zero setup - \u2705 Proven reliability - \u274c Requires internet - \u274c API costs</p>"},{"location":"advanced/local-inference/#2-speaches-local","title":"2. Speaches Local \ud83c\udfe0","text":"<p><pre><code>{ \"transcription\": { \"backend\": \"speaches\" } }\n</code></pre> - \u2705 100% offline - \u2705 Zero API costs - \u2705 Complete privacy - \u274c Requires Docker</p>"},{"location":"advanced/local-inference/#3-speaches-remote","title":"3. Speaches Remote \ud83c\udf10","text":"<p><pre><code>{ \n  \"transcription\": { \n    \"backend\": \"speaches\",\n    \"speaches\": {\n      \"url\": \"http://your-server:8000/v1\"\n    }\n  }\n}\n</code></pre> - \u2705 Dedicated resources - \u2705 Multi-user support - \u2705 GPU acceleration - \u274c Requires server setup</p>"},{"location":"advanced/local-inference/#docker-setup-local","title":"Docker Setup (Local)","text":"<p>docker-compose.speaches.yml: <pre><code>services:\n  speaches:\n    image: ghcr.io/speaches-ai/speaches:latest-cpu\n    restart: unless-stopped\n    ports:\n      - \"8000:8000\"\n    volumes:\n      - ./hf-cache:/home/ubuntu/.cache/huggingface/hub\n    environment:\n      # Keep model loaded in memory forever (zero-latency transcription)\n      - STT_MODEL_TTL=-1\n      # CPU inference configuration\n      - WHISPER__INFERENCE_DEVICE=cpu\n      - WHISPER__COMPUTE_TYPE=int8\n      - WHISPER__CPU_THREADS=8\n      - WHISPER__USE_BATCHED_MODE=true\n    deploy:\n      resources:\n        limits:\n          cpus: '8'\n          memory: 4G\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"--fail\", \"http://localhost:8000/health\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n</code></pre></p> <p>Commands: <pre><code>docker compose -f docker-compose.speaches.yml up -d\ncurl http://localhost:8000/health\n</code></pre></p>"},{"location":"advanced/local-inference/#model-selection","title":"Model Selection","text":"Model Size Memory Use Case Speed tiny 75 MB ~273 MB Fast, lower accuracy Very fast base 142 MB ~388 MB Good balance \u2b50 Fast small 466 MB ~852 MB Better accuracy Medium medium 1.5 GB ~2.1 GB High accuracy Slower large-v3 2.9 GB ~3.9 GB Best accuracy Slowest <p>Recommendation: Use <code>base</code> for voice dictation (fast + good accuracy)</p>"},{"location":"advanced/local-inference/#implementation-phases","title":"Implementation Phases","text":""},{"location":"advanced/local-inference/#phase-41-research-architecture-completed","title":"\u2705 Phase 4.1: Research &amp; Architecture (COMPLETED)","text":"<ul> <li> Research Speaches capabilities</li> <li> Validate OpenAI compatibility</li> <li> Design configuration architecture</li> </ul> <p>Status: Completed - Speaches validated as best solution</p>"},{"location":"advanced/local-inference/#phase-42-configuration-support-completed","title":"\u2705 Phase 4.2: Configuration Support (COMPLETED)","text":"<p>Goal: Add configuration-based backend selection</p> <p>Implemented Config Schema: <pre><code>{\n  \"transcription\": {\n    \"backend\": \"openai\",\n    \"openai\": {\n      \"apiKey\": \"sk-...\",\n      \"model\": \"whisper-1\"\n    },\n    \"speaches\": {\n      \"url\": \"http://localhost:8000/v1\",\n      \"apiKey\": \"none\",\n      \"model\": \"Systran/faster-whisper-base\"\n    }\n  }\n}\n</code></pre></p> <p>Completed Tasks: - [x] Add <code>transcription.backend</code> field ('openai' | 'speaches') - [x] Add <code>transcription.speaches.url</code> and <code>transcription.speaches.model</code> - [x] Add <code>transcription.openai.apiKey</code> and <code>transcription.openai.model</code> - [x] Update <code>Config.getTranscriptionConfig()</code> to return all fields - [x] Add URL validation for Speaches (<code>validateSpeachesUrl()</code>) - [x] Update <code>config.example.json</code> with new structure - [x] Add tests for new config fields - [x] Add <code>benchmarkMode</code> for side-by-side comparison</p> <p>Files Modified:  - \u2705 <code>src/config/config.ts</code> - Full implementation - \u2705 <code>config.example.json</code> - Updated with new schema - \u2705 <code>src/config/config.test.ts</code> - Tests added</p>"},{"location":"advanced/local-inference/#phase-43-transcriptionservice-refactor-completed","title":"\u2705 Phase 4.3: TranscriptionService Refactor (COMPLETED)","text":"<p>Goal: Lazy initialization + unified transcription method</p> <p>Implemented Architecture: - \u2705 Lazy client initialization via <code>getClient(backend)</code> - \u2705 <code>initializeSpeaches()</code> with proper async/await and error handling - \u2705 <code>loadSpeachesModel()</code> for model preloading - \u2705 Single unified <code>transcribe()</code> method (no separate methods) - \u2705 <code>warmup()</code> method for startup model preloading</p> <p>Completed Tasks: - [x] Add <code>getClient(backend: 'openai' | 'speaches')</code> method - [x] Implement lazy initialization (clients created on-demand) - [x] Add <code>initializeSpeaches()</code> with full error handling - [x] Add <code>loadSpeachesModel()</code> with POST to <code>/v1/models/{model}</code> - [x] Single <code>transcribe()</code> method supporting both backends - [x] Add <code>warmup()</code> for preloading at startup - [x] Proper error propagation and logging - [x] Add tests for both backends - [x] Add tests for lazy initialization - [x] Add tests for error handling</p> <p>Implementation Details: <pre><code>// Lazy initialization - clients created only when needed\nprivate async getClient(backend: \"openai\" | \"speaches\"): Promise&lt;...&gt;\n\n// Preload Speaches model at startup (called from VoiceTranscriberApp)\npublic async warmup(forceSpeaches = false): Promise&lt;...&gt;\n\n// Single transcribe method - backend determined by config\npublic async transcribe(filePath: string): Promise&lt;...&gt;\n</code></pre></p> <p>Files Modified:  - \u2705 <code>src/services/transcription.ts</code> - Full refactor - \u2705 <code>src/services/transcription.test.ts</code> - Comprehensive tests - \u2705 <code>src/index.ts</code> - Calls <code>warmup()</code> on startup</p>"},{"location":"advanced/local-inference/#phase-43b-speaches-model-preloading-completed","title":"\u2705 Phase 4.3b: Speaches Model Preloading (COMPLETED)","text":"<p>Goal: Keep model loaded in memory for zero-latency transcription</p> <p>Implementation: 1. Application-level preloading:     - <code>TranscriptionService.warmup()</code> called at app startup    - <code>loadSpeachesModel()</code> POSTs to <code>/v1/models/{model}</code>    - Conditional preload (Speaches backend OR benchmark mode)</p> <ol> <li>Docker-level persistence:</li> <li><code>STT_MODEL_TTL=-1</code> in environment variables (never unload)</li> <li>Healthcheck validates server availability</li> </ol> <p>Completed Tasks: - [x] Implement <code>loadSpeachesModel()</code> with POST to model endpoint - [x] Add <code>warmup()</code> method for startup preloading - [x] Call <code>warmup()</code> from main app when using Speaches - [x] Add <code>STT_MODEL_TTL=-1</code> to docker-compose environment - [x] Docker healthcheck for server availability - [x] Cache directory mounted (<code>./hf-cache</code>)</p> <p>Files Modified:  - \u2705 <code>src/services/transcription.ts</code> - <code>warmup()</code> and <code>loadSpeachesModel()</code> - \u2705 <code>src/index.ts</code> - Calls <code>warmup()</code> on startup - \u2705 <code>docker-compose.speaches.yml</code> - All environment variables included</p>"},{"location":"advanced/local-inference/#phase-43c-main-application-integration-completed","title":"\u2705 Phase 4.3c: Main Application Integration (COMPLETED)","text":"<p>Goal: Clean architecture with unified processing</p> <p>Implementation: - \u2705 Single <code>processAudioFile()</code> for normal mode - \u2705 Separate <code>processBenchmark()</code> for comparison mode - \u2705 No legacy methods (clean architecture from start) - \u2705 Benchmark mode creates two TranscriptionService instances - \u2705 Detailed comparison metrics (performance, similarity, differences)</p> <p>Completed Tasks: - [x] Implement <code>processAudioFile()</code> using unified transcription - [x] Implement <code>processBenchmark()</code> for side-by-side comparison - [x] Add similarity analysis (Levenshtein distance) - [x] Add text difference detection - [x] Choose best result automatically (longest transcription) - [x] Update tests for new architecture</p> <p>Architecture: <pre><code>// Normal mode: Uses configured backend\nasync processAudioFile(filePath: string): Promise&lt;void&gt;\n\n// Benchmark mode: Compares both backends side-by-side\nasync processBenchmark(filePath: string): Promise&lt;void&gt;\n</code></pre></p> <p>Files Modified:  - \u2705 <code>src/services/audio-processor.ts</code> - Both modes implemented - \u2705 <code>src/services/audio-processor.test.ts</code> - Tests for both modes - \u2705 <code>src/index.ts</code> - Conditional logic based on <code>benchmarkMode</code> - \u2705 <code>src/utils/text-similarity.ts</code> - Similarity utilities</p>"},{"location":"advanced/local-inference/#phase-44-documentation-partial","title":"\u26a0\ufe0f Phase 4.4: Documentation (PARTIAL)","text":"<p>Goal: Complete user-facing documentation</p> <p>Completed: - [x] <code>docker-compose.speaches.yml</code> with environment variables - [x] <code>config.example.json</code> with new schema - [x] Code documentation (JSDoc comments) - [x] This roadmap document</p> <p>Missing: - [ ] Create <code>docs/SPEACHES_SETUP.md</code> guide - [ ] Update main <code>README.md</code> with Speaches section - [ ] Add troubleshooting guide for Speaches - [ ] Document benchmark mode usage - [ ] Add performance comparison data</p> <p>Priority: \ud83d\udd25 High - Users need setup instructions</p>"},{"location":"advanced/local-inference/#phase-45-testing-validation-completed","title":"\u2705 Phase 4.5: Testing &amp; Validation (COMPLETED)","text":"<p>Completed: - [x] Unit tests for TranscriptionService - [x] Unit tests for Config - [x] Unit tests for AudioProcessor - [x] Tests for both OpenAI and Speaches backends - [x] Tests for lazy initialization - [x] Tests for error handling - [x] Tests for benchmark mode - [x] Similarity calculation tests</p> <p>Test Coverage: - \u2705 <code>src/config/config.test.ts</code> - Configuration validation - \u2705 <code>src/services/transcription.test.ts</code> - Backend switching - \u2705 <code>src/services/audio-processor.test.ts</code> - Processing workflows - \u2705 <code>src/utils/text-similarity.test.ts</code> - Comparison utilities</p> <p>Manual Testing Needed: - [ ] Real Speaches deployment test - [ ] Model loading performance benchmarks - [ ] Multi-language validation - [ ] Long audio file tests</p>"},{"location":"advanced/local-inference/#phase-46-release-planned","title":"\ud83d\udccb Phase 4.6: Release (PLANNED)","text":"<p>Goal: Prepare for production release</p> <p>Tasks: - [ ] Complete documentation (Phase 4.4) - [ ] Manual testing with real Speaches instance - [ ] Performance benchmarking documentation - [ ] Update CHANGELOG.md - [ ] Version bump to 0.3.0 - [ ] Git tag and release notes - [ ] Update README badges/status</p> <p>Blocked by: Phase 4.4 (Documentation)</p>"},{"location":"advanced/local-inference/#implementation-timeline","title":"Implementation Timeline","text":"<p>Total Duration: ~4 days (3.5 days completed)</p> Phase Estimated Actual Status 4.1 Research 1 day 1 day \u2705 Done 4.2 Config 0.5 days 0.5 days \u2705 Done 4.3 Service 0.5 days 1 day \u2705 Done (more comprehensive) 4.3b Preload - 0.5 days \u2705 Done (added scope) 4.3c Integration - 0.5 days \u2705 Done (added scope) 4.4 Docs 1 day - \u26a0\ufe0f Partial 4.5 Testing 1 day 0.5 days \u2705 Done 4.6 Release 0.5 days - \ud83d\udccb Planned <p>Progress: 85% complete (implementation done, documentation pending)</p>"},{"location":"advanced/local-inference/#additional-features-implemented","title":"Additional Features Implemented","text":""},{"location":"advanced/local-inference/#benchmark-mode","title":"\ud83d\udd2c Benchmark Mode","text":"<ul> <li>Compare OpenAI and Speaches side-by-side</li> <li>Performance metrics (speed, duration)</li> <li>Text similarity analysis (Levenshtein distance)</li> <li>Word-level difference detection</li> <li>Automatic best result selection</li> <li>Enable via <code>\"benchmarkMode\": true</code> in config</li> </ul>"},{"location":"advanced/local-inference/#text-similarity-analysis","title":"\ud83c\udfaf Text Similarity Analysis","text":"<ul> <li>Levenshtein distance calculation</li> <li>Character-level and word-level comparison</li> <li>Difference highlighting</li> <li>Similarity percentage scoring</li> </ul>"},{"location":"advanced/local-inference/#zero-latency-transcription","title":"\u26a1 Zero-Latency Transcription","text":"<ul> <li>Model preloading at startup</li> <li>Persistent model in Docker (STT_MODEL_TTL=-1)</li> <li>Lazy client initialization</li> <li>Minimal overhead for second+ transcriptions</li> </ul>"},{"location":"advanced/local-inference/#known-issues-limitations","title":"Known Issues &amp; Limitations","text":""},{"location":"advanced/local-inference/#current-limitations","title":"Current Limitations:","text":"<ol> <li>Documentation: Setup guides incomplete (Phase 4.4)</li> <li>Manual Testing: No real-world Speaches deployment tested yet</li> <li>Performance Data: No documented benchmarks yet</li> </ol>"},{"location":"advanced/local-inference/#future-improvements","title":"Future Improvements:","text":"<ol> <li>GPU support documentation</li> <li>Multiple model support (runtime switching)</li> <li>Remote Speaches server examples</li> <li>Performance tuning guide</li> <li>Cost analysis (OpenAI vs self-hosted)</li> </ol>"},{"location":"advanced/local-inference/#next-steps","title":"Next Steps","text":""},{"location":"advanced/local-inference/#immediate-phase-44-documentation","title":"Immediate (Phase 4.4 - Documentation):","text":"<ol> <li>Create <code>docs/SPEACHES_SETUP.md</code> comprehensive guide</li> <li>Update main <code>README.md</code> with Speaches section</li> <li>Add troubleshooting section</li> <li>Document benchmark mode</li> </ol>"},{"location":"advanced/local-inference/#before-release-phase-46","title":"Before Release (Phase 4.6):","text":"<ol> <li>Manual test with real Speaches deployment</li> <li>Run benchmark mode with sample audio</li> <li>Document performance results</li> <li>Update CHANGELOG</li> <li>Version bump and release</li> </ol>"},{"location":"advanced/local-inference/#references","title":"References","text":"<ul> <li>Speaches GitHub</li> <li>faster-whisper</li> <li>OpenAI API Docs</li> <li>Docker Docs</li> <li>Levenshtein Distance</li> </ul> <p>Version: v3.0 (Updated with actual implementation status) Last Updated: 2025-01-11 Project Status: \ud83d\udfe2 Implementation Complete - Documentation Pending</p>"},{"location":"advanced/speaches-integration/","title":"Speaches Integration Guide","text":"<p>Complete guide for setting up self-hosted transcription with Speaches.</p>"},{"location":"advanced/speaches-integration/#why-speaches","title":"Why Speaches?","text":"<ul> <li> <p>\ud83d\udcb0 Zero Cost</p> <p>No API fees - unlimited transcriptions for free</p> </li> <li> <p>\ud83d\udd12 Complete Privacy</p> <p>100% offline - audio never leaves your machine</p> </li> <li> <p>\u26a1 Same Speed</p> <p>Base model performs identically to OpenAI (3.7s vs 3.8s)</p> </li> <li> <p>\ud83c\udfaf High Accuracy</p> <p>91-100% text similarity with OpenAI depending on model</p> </li> </ul>"},{"location":"advanced/speaches-integration/#quick-setup","title":"Quick Setup","text":""},{"location":"advanced/speaches-integration/#step-1-create-docker-compose-file","title":"Step 1: Create Docker Compose File","text":"<p>Create <code>docker-compose.speaches.yml</code>:</p> <pre><code>services:\n  speaches:\n    image: ghcr.io/speaches-ai/speaches:latest-cpu\n    ports:\n      - \"8000:8000\"\n    volumes:\n      - ./hf-cache:/home/ubuntu/.cache/huggingface/hub\n    environment:\n      - STT_MODEL_TTL=-1  # Keep model in memory\n      - WHISPER__INFERENCE_DEVICE=cpu\n      - WHISPER__COMPUTE_TYPE=int8\n      - WHISPER__CPU_THREADS=8\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/health\"]\n</code></pre>"},{"location":"advanced/speaches-integration/#step-2-start-speaches","title":"Step 2: Start Speaches","text":"<pre><code>docker compose -f docker-compose.speaches.yml up -d\n</code></pre> <p>First startup: Downloads model (~140MB for base) - takes 1-2 minutes</p>"},{"location":"advanced/speaches-integration/#step-3-configure-voice-transcriber","title":"Step 3: Configure Voice Transcriber","text":"<p>Edit config:</p> <pre><code>nano ~/.config/voice-transcriber/config.json\n</code></pre> <p>Update:</p> <pre><code>{\n  \"language\": \"fr\",\n  \"formatterEnabled\": false,\n  \"transcription\": {\n    \"backend\": \"speaches\",\n    \"speaches\": {\n      \"url\": \"http://localhost:8000/v1\",\n      \"apiKey\": \"none\",\n      \"model\": \"Systran/faster-whisper-base\"\n    }\n  }\n}\n</code></pre>"},{"location":"advanced/speaches-integration/#step-4-restart-application","title":"Step 4: Restart Application","text":"<pre><code># Restart Voice Transcriber\nvoice-transcriber\n</code></pre> <p>\u2705 Done! First transcription will auto-download the model.</p>"},{"location":"advanced/speaches-integration/#available-models","title":"Available Models","text":"Model Size Memory Speed Accuracy Use Case tiny 75 MB ~273 MB \u26a1\u26a1\u26a1 \u2b50\u2b50 Quick testing base \u2b50 142 MB ~388 MB \u26a1\u26a1 \u2b50\u2b50\u2b50 Recommended small 466 MB ~852 MB \u26a1 \u2b50\u2b50\u2b50\u2b50 Better accuracy medium 1.5 GB ~2.1 GB \ud83d\udc22 \u2b50\u2b50\u2b50\u2b50\u2b50 High accuracy large-v3 2.9 GB ~3.9 GB \ud83d\udc22\ud83d\udc22 \u2b50\u2b50\u2b50\u2b50\u2b50 Maximum accuracy <p>Recommendation: Base Model</p> <ul> <li>Comparable speed to OpenAI (0.97x)</li> <li>91% accuracy - excellent for daily use</li> <li>Low resource usage (~400MB RAM)</li> <li>Zero cost</li> </ul>"},{"location":"advanced/speaches-integration/#performance-comparison","title":"Performance Comparison","text":"<p>Benchmark: 30s French audio, Remote server (8 CPU / 8GB RAM)</p> <pre><code>Model: base\nOpenAI:   3.70s\nSpeaches: 3.81s\nSpeedup:  0.97x (nearly identical!)\nAccuracy: 91.4%\n</code></pre>"},{"location":"advanced/speaches-integration/#changing-models","title":"Changing Models","text":"<p>Edit config to use different model:</p> <pre><code>{\n  \"transcription\": {\n    \"backend\": \"speaches\",\n    \"speaches\": {\n      \"model\": \"Systran/faster-whisper-small\"\n    }\n  }\n}\n</code></pre> <p>Available models: - <code>Systran/faster-whisper-tiny</code> - <code>Systran/faster-whisper-base</code> \u2b50 - <code>Systran/faster-whisper-small</code> - <code>Systran/faster-whisper-medium</code> - <code>Systran/faster-whisper-large-v3</code></p> <p>Restart application for changes to take effect.</p>"},{"location":"advanced/speaches-integration/#gpu-acceleration","title":"GPU Acceleration","text":"<p>For significantly faster processing with medium/large models:</p> <pre><code>services:\n  speaches:\n    image: ghcr.io/speaches-ai/speaches:latest-cuda  # GPU image\n    runtime: nvidia\n    environment:\n      - WHISPER__INFERENCE_DEVICE=cuda\n      - WHISPER__COMPUTE_TYPE=float16\n</code></pre> <p>Requirements: NVIDIA GPU with CUDA support</p>"},{"location":"advanced/speaches-integration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"advanced/speaches-integration/#model-download-fails","title":"Model Download Fails","text":"<pre><code># Check logs\ndocker compose -f docker-compose.speaches.yml logs -f speaches\n</code></pre> <p>Solution: Ensure internet connection for initial download</p>"},{"location":"advanced/speaches-integration/#service-not-responding","title":"Service Not Responding","text":"<pre><code># Check health\ncurl http://localhost:8000/health\n\n# Restart service\ndocker compose -f docker-compose.speaches.yml restart\n</code></pre>"},{"location":"advanced/speaches-integration/#out-of-memory","title":"Out of Memory","text":"<p>Solution: Use smaller model or increase Docker memory limit</p> <pre><code>services:\n  speaches:\n    deploy:\n      resources:\n        limits:\n          memory: 4G  # Increase memory\n</code></pre>"},{"location":"advanced/speaches-integration/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"advanced/speaches-integration/#custom-whisper-parameters","title":"Custom Whisper Parameters","text":"<pre><code>environment:\n  - WHISPER__BEAM_SIZE=5\n  - WHISPER__BEST_OF=5\n  - WHISPER__TEMPERATURE=0.0\n</code></pre>"},{"location":"advanced/speaches-integration/#remote-speaches-server","title":"Remote Speaches Server","text":"<p>Run Speaches on a VPS and connect remotely:</p> <pre><code>{\n  \"transcription\": {\n    \"backend\": \"speaches\",\n    \"speaches\": {\n      \"url\": \"https://your-server.com/v1\",\n      \"apiKey\": \"your-api-key\",\n      \"model\": \"Systran/faster-whisper-base\"\n    }\n  }\n}\n</code></pre>"},{"location":"advanced/speaches-integration/#cost-comparison","title":"Cost Comparison","text":"<p>OpenAI Whisper: - $0.006 per minute - 100 hours = $36/month - No local resources needed</p> <p>Speaches (Self-Hosted): - \\(0 transcription cost - VPS: ~\\)5-10/month (optional) - Requires local/VPS resources</p> <p>Break-Even Point</p> <p>After ~100 hours of transcription, Speaches becomes more cost-effective than OpenAI.</p>"},{"location":"advanced/speaches-integration/#next-steps","title":"Next Steps","text":"<ul> <li>Whisper Models Comparison - Detailed model benchmarks</li> <li>Transcription Backends - Backend comparison</li> <li>Configuration Guide - Advanced settings</li> </ul> <p>Need Help?</p> <ul> <li>Speaches Documentation</li> <li>Voice Transcriber Issues</li> </ul>"},{"location":"advanced/whisper-models/","title":"Faster-Whisper Models Comparison","text":""},{"location":"advanced/whisper-models/#model-variants-overview","title":"Model Variants Overview","text":"Model Parameters Multilingual English-Only Distilled tiny 39M \u2705 \u2705 <code>.en</code> \u274c base 74M \u2705 \u2705 <code>.en</code> \u274c small 244M \u2705 \u2705 <code>.en</code> \u2705 <code>distil-*.en</code> medium 769M \u2705 \u2705 <code>.en</code> \u2705 <code>distil-*.en</code> large-v3 1550M \u2705 \u274c \u2705 <code>distil-*</code>"},{"location":"advanced/whisper-models/#quick-selection-guide","title":"Quick Selection Guide","text":""},{"location":"advanced/whisper-models/#base-recommended-default","title":"Base (recommended default) \u2b50","text":"<ul> <li>Good balance speed/accuracy</li> <li>74M parameters</li> <li>Use for: voice dictation, development</li> </ul>"},{"location":"advanced/whisper-models/#small-better-quality","title":"Small (better quality)","text":"<ul> <li>Better accuracy</li> <li>244M parameters</li> <li>Use for: production, professional apps</li> </ul>"},{"location":"advanced/whisper-models/#medium-high-quality","title":"Medium (high quality)","text":"<ul> <li>High accuracy</li> <li>769M parameters</li> <li>Use for: meetings, subtitling</li> </ul>"},{"location":"advanced/whisper-models/#large-v3-maximum-quality","title":"Large-v3 (maximum quality)","text":"<ul> <li>Best accuracy</li> <li>1550M parameters</li> <li>Use for: critical apps (legal, medical)</li> </ul>"},{"location":"advanced/whisper-models/#distilled-distil-","title":"Distilled (<code>distil-*</code>)","text":"<ul> <li>30-50% faster</li> <li>Similar accuracy (~1-3% WER difference)</li> <li>Available: small.en, medium.en, large-v2, large-v3</li> </ul>"},{"location":"advanced/whisper-models/#english-only-en","title":"English-only (<code>.en</code>)","text":"<ul> <li>English transcription only</li> <li>20-30% faster than multilingual</li> <li>Better English accuracy</li> </ul>"},{"location":"advanced/whisper-models/#performance-comparison","title":"Performance Comparison","text":"Model CPU Time* Memory (RAM) Speed Accuracy tiny 1x ~1 GB \u26a1\u26a1\u26a1\u26a1\u26a1 \u2b50\u2b50 base 2x ~1 GB \u26a1\u26a1\u26a1\u26a1 \u2b50\u2b50\u2b50 small 5x ~2 GB \u26a1\u26a1\u26a1 \u2b50\u2b50\u2b50\u2b50 medium 12x ~5 GB \u26a1\u26a1 \u2b50\u2b50\u2b50\u2b50\u2b50 large-v3 30x ~10 GB \u26a1 \u2b50\u2b50\u2b50\u2b50\u2b50\u2b50 <p>*Relative to tiny model on 1 minute of audio</p>"},{"location":"advanced/whisper-models/#speaches-integration","title":"Speaches Integration","text":"<p>Dynamic model loading (specify in API request):</p> <pre><code>// In transcription.ts\nconst transcription = await openai.audio.transcriptions.create({\n  file: audioFile,\n  model: 'base',  // or 'small', 'medium', 'large-v3', etc.\n  language: 'en'\n});\n</code></pre> <p>Available models: - <code>tiny</code>, <code>tiny.en</code> - <code>base</code>, <code>base.en</code> - <code>small</code>, <code>small.en</code> - <code>medium</code>, <code>medium.en</code> - <code>large-v1</code>, <code>large-v2</code>, <code>large-v3</code> - <code>distil-small.en</code>, <code>distil-medium.en</code> - <code>distil-large-v2</code>, <code>distil-large-v3</code></p> <p>Models are auto-downloaded on first use.</p>"},{"location":"advanced/whisper-models/#recommendations-by-use-case","title":"Recommendations by Use Case","text":"Use Case Model Why Voice dictation <code>base</code> or <code>small</code> Fast + good accuracy Meetings <code>medium</code> High accuracy + multi-speaker Real-time <code>tiny.en</code> or <code>base.en</code> Low latency Critical <code>large-v3</code> Maximum accuracy Development <code>base</code> Fast iteration"},{"location":"advanced/whisper-models/#language-support","title":"Language Support","text":"<p>Multilingual (99+ languages): - Western European: en, fr, de, es, it, pt - Eastern European: ru, pl, uk, cs - Asian: zh, ja, ko, hi, ar</p> <p>English-only (<code>.en</code> suffix): - Cannot transcribe other languages - 20-30% faster for English - Smaller model size</p>"},{"location":"advanced/whisper-models/#configuration","title":"Configuration","text":"<pre><code>{\n  \"openaiApiKey\": \"not-needed-for-speaches\",\n  \"inferenceBaseUrl\": \"http://localhost:8000/v1\",\n  \"language\": \"en\",\n  \"whisperModel\": \"base\"\n}\n</code></pre>"},{"location":"advanced/whisper-models/#references","title":"References","text":"<ul> <li>Faster-Whisper GitHub</li> <li>Hugging Face Collection</li> <li>Speaches</li> </ul> <p>Version: v2.0 Last Updated: 2025-10-11</p>"},{"location":"development/api-reference/","title":"Voice Transcriber - API Reference","text":""},{"location":"development/api-reference/#overview","title":"Overview","text":"<p>This document provides detailed API reference for all services and interfaces in the Voice Transcriber application. All services follow consistent patterns with simple interfaces and standardized error handling.</p>"},{"location":"development/api-reference/#common-patterns","title":"Common Patterns","text":""},{"location":"development/api-reference/#result-interface","title":"Result Interface","text":"<p>All service methods return a consistent result interface:</p> <pre><code>interface ServiceResult {\n  success: boolean;\n  error?: string;\n  // Additional data fields specific to the operation\n}\n</code></pre>"},{"location":"development/api-reference/#error-handling","title":"Error Handling","text":"<ul> <li>Services never throw exceptions in normal operation</li> <li>All errors are returned via the result interface</li> <li>Error messages are user-friendly and actionable</li> <li>Logging is handled internally by each service</li> </ul>"},{"location":"development/api-reference/#core-application","title":"Core Application","text":""},{"location":"development/api-reference/#voicetranscriberapp","title":"VoiceTranscriberApp","text":"<p>Main application class that orchestrates all services.</p> <pre><code>class VoiceTranscriberApp {\n  constructor(configPath?: string)\n\n  // Lifecycle Methods\n  initialize(): Promise&lt;{ success: boolean; error?: string }&gt;\n  shutdown(): Promise&lt;void&gt;\n\n  // Private Event Handlers (called by system tray)\n  private handleRecordingStart(): Promise&lt;void&gt;\n  private handleRecordingStop(): Promise&lt;void&gt;\n  private handleQuit(): Promise&lt;void&gt;\n  private processAudioFile(filePath: string): Promise&lt;void&gt;\n}\n</code></pre>"},{"location":"development/api-reference/#methods","title":"Methods","text":"<p><code>initialize()</code> - Loads configuration with setup wizard if needed - Initializes all services with proper dependency injection - Sets up system tray with event callbacks - Returns initialization result</p> <p><code>shutdown()</code> - Stops any active recording - Cleanly shuts down system tray - Performs cleanup operations</p>"},{"location":"development/api-reference/#configuration-service","title":"Configuration Service","text":""},{"location":"development/api-reference/#config","title":"Config","text":"<p>Manages application configuration with user-friendly setup.</p> <pre><code>interface ConfigData {\n  openaiApiKey: string;\n  formatterEnabled: boolean;\n}\n\nclass Config {\n  openaiApiKey: string;\n  formatterEnabled: boolean;\n\n  constructor(configPath?: string)\n\n  // Configuration Methods\n  load(): Promise&lt;void&gt;\n  loadWithSetup(): Promise&lt;void&gt;\n  save(): Promise&lt;void&gt;\n\n  // Private Methods\n  private setupWizard(): Promise&lt;void&gt;\n  private promptForApiKey(): Promise&lt;string&gt;\n  private getUserConfigPath(): string\n  private getUserConfigDir(): string\n}\n</code></pre>"},{"location":"development/api-reference/#methods_1","title":"Methods","text":"<p><code>load()</code> - Loads configuration from JSON file - Uses defaults if file doesn't exist or is invalid - Silent failure with default values</p> <p><code>loadWithSetup()</code> - Loads configuration - Runs setup wizard for first-time users - Creates config directory and file as needed</p> <p><code>save()</code> - Saves current configuration to JSON file - Creates config directory if needed - Overwrites existing configuration</p> <p><code>setupWizard()</code> - Interactive first-run setup - Prompts for OpenAI API key - Creates initial configuration file</p>"},{"location":"development/api-reference/#configuration-paths","title":"Configuration Paths","text":"<ul> <li>Default: <code>~/.config/voice-transcriber/config.json</code></li> <li>Custom: Provided via constructor parameter</li> </ul>"},{"location":"development/api-reference/#system-tray-service","title":"System Tray Service","text":""},{"location":"development/api-reference/#systemtrayservice","title":"SystemTrayService","text":"<p>Manages system tray integration with visual state feedback.</p> <pre><code>enum TrayState {\n  IDLE = \"idle\",\n  RECORDING = \"recording\",\n  PROCESSING = \"processing\"\n}\n\ninterface TrayConfig {\n  callbacks: {\n    onRecordingStart: () =&gt; void;\n    onRecordingStop: () =&gt; void;\n    onQuit: () =&gt; void;\n  };\n}\n\ninterface TrayResult {\n  success: boolean;\n  error?: string;\n}\n\nclass SystemTrayService {\n  constructor(config: TrayConfig, systrayConstructor?: typeof SysTray)\n\n  // Public Methods\n  initialize(): Promise&lt;TrayResult&gt;\n  setState(state: TrayState): Promise&lt;TrayResult&gt;\n  shutdown(): Promise&lt;TrayResult&gt;\n\n  // Private Methods\n  private getIconBase64(state: TrayState): string\n  private getTooltip(state: TrayState): string\n}\n</code></pre>"},{"location":"development/api-reference/#methods_2","title":"Methods","text":"<p><code>initialize()</code> - Creates system tray with menu items - Sets up click event handlers - Waits for tray to be ready - Returns initialization result</p> <p><code>setState(state: TrayState)</code> - Updates tray icon based on application state - Modifies menu item availability - Updates tooltip text - Handles icon recreation for state changes</p> <p><code>shutdown()</code> - Cleanly destroys system tray - Releases system resources</p>"},{"location":"development/api-reference/#states-and-icons","title":"States and Icons","text":"<ul> <li>IDLE: Green circle - Ready to record</li> <li>RECORDING: Red circle - Actively recording</li> <li>PROCESSING: Purple circle - Transcribing audio</li> </ul>"},{"location":"development/api-reference/#menu-items","title":"Menu Items","text":"<ul> <li>\ud83c\udfa4 Start Recording: Enabled when IDLE</li> <li>\u23f9\ufe0f Stop Recording: Enabled when RECORDING</li> <li>\u274c Exit: Always enabled</li> </ul>"},{"location":"development/api-reference/#audio-recording-service","title":"Audio Recording Service","text":""},{"location":"development/api-reference/#audiorecorder","title":"AudioRecorder","text":"<p>Handles system audio capture using Linux arecord.</p> <pre><code>interface AudioRecorderConfig {\n  tempDir?: string;\n}\n\ninterface RecordingResult {\n  success: boolean;\n  filePath?: string;\n  error?: string;\n}\n\nclass AudioRecorder {\n  constructor(config?: AudioRecorderConfig)\n\n  // Public Methods\n  startRecording(): Promise&lt;RecordingResult&gt;\n  stopRecording(): Promise&lt;RecordingResult&gt;\n  isRecording(): boolean\n}\n</code></pre>"},{"location":"development/api-reference/#methods_3","title":"Methods","text":"<p><code>startRecording()</code> - Creates temporary directory if needed - Generates timestamped filename - Spawns arecord process with CD quality settings - Returns recording result with file path</p> <p><code>stopRecording()</code> - Sends SIGTERM to arecord process - Cleans up process references - Returns result with final file path</p> <p><code>isRecording()</code> - Returns true if recording process is active - Used for state validation</p>"},{"location":"development/api-reference/#audio-format","title":"Audio Format","text":"<ul> <li>Format: WAV (CD quality)</li> <li>Sample Rate: 44.1kHz</li> <li>Bit Depth: 16-bit</li> <li>Channels: Stereo</li> <li>Device: ALSA default input</li> </ul>"},{"location":"development/api-reference/#file-management","title":"File Management","text":"<ul> <li>Location: <code>/tmp/transcriber/recording-{timestamp}.wav</code></li> <li>Naming: ISO timestamp with safe characters</li> <li>Cleanup: Manual cleanup required (handled by main app)</li> </ul>"},{"location":"development/api-reference/#transcription-service","title":"Transcription Service","text":""},{"location":"development/api-reference/#transcriptionservice","title":"TranscriptionService","text":"<p>Converts audio files to text using OpenAI Whisper API.</p> <pre><code>interface TranscriptionConfig {\n  apiKey: string;\n  language?: string;\n  prompt?: string;\n}\n\ninterface TranscriptionResult {\n  success: boolean;\n  text?: string;\n  error?: string;\n}\n\nclass TranscriptionService {\n  constructor(config: TranscriptionConfig)\n\n  // Public Methods\n  transcribe(filePath: string): Promise&lt;TranscriptionResult&gt;\n}\n</code></pre>"},{"location":"development/api-reference/#methods_4","title":"Methods","text":"<p><code>transcribe(filePath: string)</code> - Validates audio file exists - Creates read stream for file upload - Calls OpenAI Whisper API with optimized settings - Returns transcribed text or error</p>"},{"location":"development/api-reference/#configuration","title":"Configuration","text":"<p>Default Settings: <pre><code>{\n  language: undefined,  // Auto-detect French/English\n  prompt: \"Please transcribe this audio exactly as spoken, preserving the original language. The speaker may mix French and English in the same sentence. Keep technical terms in their original language (English), but preserve French sentence structure and grammar. Do not translate between languages.\"\n}\n</code></pre></p> <p>API Parameters: - Model: <code>whisper-1</code> (OpenAI's production model) - Language: Auto-detect if undefined - Prompt: Enhanced for French/English mixed speech</p>"},{"location":"development/api-reference/#multilingual-support","title":"Multilingual Support","text":"<ul> <li>Auto-Detection: Automatic language identification</li> <li>Mixed Speech: Preserves French/English code-switching</li> <li>Technical Terms: Keeps English technical vocabulary</li> <li>Grammar: Maintains original language sentence structure</li> </ul>"},{"location":"development/api-reference/#text-formatting-service","title":"Text Formatting Service","text":""},{"location":"development/api-reference/#formatterservice","title":"FormatterService","text":"<p>Optional text enhancement using OpenAI GPT API.</p> <pre><code>interface FormatterConfig {\n  apiKey: string;\n  enabled: boolean;\n  prompt?: string;\n}\n\ninterface FormatResult {\n  success: boolean;\n  text?: string;\n  error?: string;\n}\n\nclass FormatterService {\n  constructor(config: FormatterConfig)\n\n  // Public Methods\n  formatText(text: string): Promise&lt;FormatResult&gt;\n}\n</code></pre>"},{"location":"development/api-reference/#methods_5","title":"Methods","text":"<p><code>formatText(text: string)</code> - Returns original text if formatting disabled - Validates input text is not empty - Calls OpenAI GPT API for text enhancement - Returns formatted text or error</p>"},{"location":"development/api-reference/#configuration_1","title":"Configuration","text":"<p>Default Settings: <pre><code>{\n  prompt: \"Please format this transcribed text with proper grammar and punctuation. The text may be in French or English - preserve the original language:\",\n  enabled: true\n}\n</code></pre></p> <p>API Parameters: - Model: <code>gpt-3.5-turbo</code> (fast and cost-effective) - Temperature: 0.3 (consistent, low-creativity output) - Max Tokens: 1000 (sufficient for typical transcriptions)</p>"},{"location":"development/api-reference/#text-enhancement","title":"Text Enhancement","text":"<ul> <li>Grammar: Corrects grammatical errors</li> <li>Punctuation: Adds proper punctuation</li> <li>Language Preservation: Maintains original language</li> <li>Tone: Preserves original speaking style</li> </ul>"},{"location":"development/api-reference/#clipboard-service","title":"Clipboard Service","text":""},{"location":"development/api-reference/#clipboardservice","title":"ClipboardService","text":"<p>Cross-platform clipboard operations.</p> <pre><code>interface ClipboardResult {\n  success: boolean;\n  error?: string;\n}\n\nclass ClipboardService {\n  // Public Methods\n  writeText(text: string): Promise&lt;ClipboardResult&gt;\n}\n</code></pre>"},{"location":"development/api-reference/#methods_6","title":"Methods","text":"<p><code>writeText(text: string)</code> - Validates input text is not empty - Writes text to system clipboard - Returns operation result</p>"},{"location":"development/api-reference/#platform-support","title":"Platform Support","text":"<ul> <li>Linux: Uses <code>clipboardy</code> with xsel/xclip backend</li> <li>Windows: Native Windows clipboard API</li> <li>macOS: Native macOS clipboard API</li> </ul>"},{"location":"development/api-reference/#logging-service","title":"Logging Service","text":""},{"location":"development/api-reference/#logger","title":"Logger","text":"<p>Simple console-based logging utility.</p> <pre><code>interface Logger {\n  info(message: string): void\n  error(message: string): void\n}\n\nconst logger: Logger\n</code></pre>"},{"location":"development/api-reference/#methods_7","title":"Methods","text":"<p><code>info(message: string)</code> - Logs informational messages to console - Includes timestamp and formatted output</p> <p><code>error(message: string)</code> - Logs error messages to console - Includes timestamp and error formatting</p>"},{"location":"development/api-reference/#log-levels","title":"Log Levels","text":"<ul> <li>INFO: General application flow and status</li> <li>ERROR: Errors and exceptions</li> </ul>"},{"location":"development/api-reference/#output-format","title":"Output Format","text":"<pre><code>[TIMESTAMP] [LEVEL] MESSAGE\n</code></pre>"},{"location":"development/api-reference/#error-codes-and-messages","title":"Error Codes and Messages","text":""},{"location":"development/api-reference/#common-error-patterns","title":"Common Error Patterns","text":"<p>Configuration Errors: - <code>\"OpenAI API key not configured\"</code> - <code>\"Config file could not be loaded\"</code></p> <p>Audio Recording Errors: - <code>\"Already recording\"</code> - <code>\"Not recording\"</code> - <code>\"Failed to start recording: {details}\"</code></p> <p>Transcription Errors: - <code>\"Audio file does not exist\"</code> - <code>\"No transcription text received\"</code> - <code>\"Failed to transcribe audio: {details}\"</code></p> <p>System Tray Errors: - <code>\"System tray not initialized\"</code> - <code>\"Failed to initialize: {details}\"</code></p> <p>Clipboard Errors: - <code>\"Text cannot be empty\"</code> - <code>\"Failed to write to clipboard: {details}\"</code></p>"},{"location":"development/api-reference/#api-rate-limiting","title":"API Rate Limiting","text":"<p>The application does not implement automatic retry logic. Rate limiting is handled by: - Using conservative API call patterns - Single transcription per recording session - Optional formatting (can be disabled)</p>"},{"location":"development/api-reference/#network-error-handling","title":"Network Error Handling","text":"<p>Network errors are returned as operation failures: - Connection timeouts - Invalid API keys - Service unavailable - Rate limit exceeded</p>"},{"location":"development/api-reference/#usage-examples","title":"Usage Examples","text":""},{"location":"development/api-reference/#basic-application-lifecycle","title":"Basic Application Lifecycle","text":"<pre><code>// Initialize application\nconst app = new VoiceTranscriberApp();\nconst result = await app.initialize();\n\nif (!result.success) {\n  console.error(result.error);\n  process.exit(1);\n}\n\n// Application runs via system tray events\n// Shutdown when needed\nawait app.shutdown();\n</code></pre>"},{"location":"development/api-reference/#custom-configuration","title":"Custom Configuration","text":"<pre><code>// Load custom config path\nconst config = new Config('/path/to/custom/config.json');\nawait config.load();\n\n// Modify settings\nconfig.formatterEnabled = false;\nawait config.save();\n\n// Use with application\nconst app = new VoiceTranscriberApp('/path/to/custom/config.json');\n</code></pre>"},{"location":"development/api-reference/#manual-service-usage","title":"Manual Service Usage","text":"<pre><code>// Direct transcription service usage\nconst transcriber = new TranscriptionService({\n  apiKey: 'your-api-key'\n});\n\nconst result = await transcriber.transcribe('/path/to/audio.wav');\nif (result.success) {\n  console.log('Transcription:', result.text);\n}\n</code></pre>"},{"location":"development/api-reference/#error-handling-pattern","title":"Error Handling Pattern","text":"<pre><code>// Consistent error handling across all services\nconst result = await service.someOperation();\n\nif (!result.success) {\n  logger.error(`Operation failed: ${result.error}`);\n  // Handle error appropriately\n  return;\n}\n\n// Use result.data if operation succeeded\nconsole.log('Success:', result.data);\n</code></pre>"},{"location":"development/architecture/","title":"Voice Transcriber - Technical Architecture","text":""},{"location":"development/architecture/#overview","title":"Overview","text":"<p>Voice Transcriber is a lightweight desktop application that provides seamless voice-to-text conversion with system tray integration. The application follows a service-oriented architecture with clear separation of concerns and minimal dependencies.</p>"},{"location":"development/architecture/#core-architecture-principles","title":"Core Architecture Principles","text":""},{"location":"development/architecture/#1-simplicity-first","title":"1. Simplicity First","text":"<ul> <li>Each service has 3-5 core methods maximum</li> <li>Simple interfaces: <code>{ success: boolean, error?: string }</code></li> <li>No overengineering or complex retry logic</li> <li>Console logging only (info/error levels)</li> </ul>"},{"location":"development/architecture/#2-service-oriented-design","title":"2. Service-Oriented Design","text":"<ul> <li>Clear separation of concerns</li> <li>Dependency injection for testability</li> <li>Consistent error handling patterns</li> <li>Graceful degradation when services fail</li> </ul>"},{"location":"development/architecture/#3-user-centric-approach","title":"3. User-Centric Approach","text":"<ul> <li>First-run setup wizard</li> <li>Visual feedback via system tray states</li> <li>Automatic clipboard integration</li> <li>Multilingual support (French/English auto-detection)</li> </ul>"},{"location":"development/architecture/#system-components","title":"System Components","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                Main Application                     \u2502\n\u2502            (VoiceTranscriberApp)                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                  \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502         \u2502         \u2502\n        \u25bc         \u25bc         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   System    \u2502 \u2502    Audio    \u2502 \u2502   Configuration \u2502\n\u2502    Tray     \u2502 \u2502  Recording  \u2502 \u2502     Service     \u2502\n\u2502   Service   \u2502 \u2502   Service   \u2502 \u2502                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502                              \u2502\n        \u25bc                              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Clipboard  \u2502                \u2502  OpenAI API     \u2502\n\u2502   Service   \u2502                \u2502   Services      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                \u2502                 \u2502\n                               \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n                               \u2502 \u2502Transcription\u2502 \u2502\n                               \u2502 \u2502   Service   \u2502 \u2502\n                               \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n                               \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n                               \u2502 \u2502 Formatter   \u2502 \u2502\n                               \u2502 \u2502   Service   \u2502 \u2502\n                               \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n                               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"development/architecture/#service-descriptions","title":"Service Descriptions","text":""},{"location":"development/architecture/#main-application-srcindexts","title":"Main Application (<code>src/index.ts</code>)","text":"<p>Purpose: Central orchestrator that manages all services and application lifecycle.</p> <p>Key Responsibilities: - Service initialization and dependency injection - Event handling and workflow coordination - Error management and recovery - Graceful shutdown handling</p> <p>State Machine: <pre><code>IDLE \u2192 RECORDING \u2192 PROCESSING \u2192 IDLE\n  \u2191                              \u2193\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 ERROR \u2190\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p>"},{"location":"development/architecture/#system-tray-service-srcservicessystem-trayts","title":"System Tray Service (<code>src/services/system-tray.ts</code>)","text":"<p>Purpose: Manages system tray integration with visual state feedback.</p> <p>Features: - Three visual states with distinct icons:   - \ud83d\udfe2 IDLE: Green circle (ready to record)   - \ud83d\udd34 RECORDING: Red circle (actively recording)   - \ud83d\udfe3 PROCESSING: Purple circle (transcribing audio) - Click-to-record functionality - Context menu with Start/Stop/Exit options - Cross-platform icon compatibility (Base64 encoded)</p> <p>Icon Management: - Icons embedded as Base64 strings for npm distribution - Automatic menu state updates based on recording status - Workaround for node-systray-v2 double icon issues</p>"},{"location":"development/architecture/#audio-recording-service-srcservicesaudio-recorderts","title":"Audio Recording Service (<code>src/services/audio-recorder.ts</code>)","text":"<p>Purpose: Handles system audio capture using Linux arecord.</p> <p>Features: - Spawns arecord process for high-quality audio capture - Temporary file management in system temp directory - Process lifecycle management (start/stop/cleanup) - CD-quality WAV format (44.1kHz, 16-bit)</p> <p>System Dependencies: <pre><code># Required for audio recording\nsudo apt-get install alsa-utils\n</code></pre></p>"},{"location":"development/architecture/#transcription-service-srcservicestranscriptionts","title":"Transcription Service (<code>src/services/transcription.ts</code>)","text":"<p>Purpose: Converts audio files to text using OpenAI Whisper API.</p> <p>Features: - Automatic language detection (French/English mixed speech) - Enhanced prompting for preserving original language structure - Technical term preservation in mixed-language contexts - Robust error handling for API failures</p> <p>Configuration: <pre><code>{\n  apiKey: string;           // OpenAI API key\n  language?: string;        // Auto-detect if undefined\n  prompt?: string;          // Custom transcription prompt\n}\n</code></pre></p>"},{"location":"development/architecture/#formatter-service-srcservicesformatterts","title":"Formatter Service (<code>src/services/formatter.ts</code>)","text":"<p>Purpose: Optional text enhancement using OpenAI GPT API.</p> <p>Features: - Grammar and punctuation improvement - Language preservation (French/English) - Configurable enable/disable - Temperature-controlled generation (0.3 for consistency)</p>"},{"location":"development/architecture/#configuration-service-srcconfigconfigts","title":"Configuration Service (<code>src/config/config.ts</code>)","text":"<p>Purpose: Manages application configuration with user-friendly setup.</p> <p>Features: - User config directory (<code>~/.config/voice-transcriber/</code>) - Interactive first-run setup wizard - API key validation and storage - JSON-based configuration file</p> <p>Config Location: <pre><code>~/.config/voice-transcriber/config.json\n</code></pre></p>"},{"location":"development/architecture/#clipboard-service-srcservicesclipboardts","title":"Clipboard Service (<code>src/services/clipboard.ts</code>)","text":"<p>Purpose: Cross-platform clipboard operations.</p> <p>Features: - Automatic text copying after transcription - Cross-platform compatibility (Linux/Windows/macOS) - Simple success/error feedback</p>"},{"location":"development/architecture/#data-flow","title":"Data Flow","text":""},{"location":"development/architecture/#recording-workflow","title":"Recording Workflow","text":"<pre><code>1. User clicks tray icon\n   \u2193\n2. System tray \u2192 RECORDING state\n   \u2193\n3. Audio recorder starts arecord process\n   \u2193\n4. User clicks again to stop\n   \u2193\n5. System tray \u2192 PROCESSING state\n   \u2193\n6. Audio file saved to temp directory\n   \u2193\n7. Transcription service \u2192 OpenAI Whisper API\n   \u2193\n8. [Optional] Formatter service \u2192 OpenAI GPT API\n   \u2193\n9. Clipboard service writes final text\n   \u2193\n10. System tray \u2192 IDLE state\n</code></pre>"},{"location":"development/architecture/#error-handling-flow","title":"Error Handling Flow","text":"<pre><code>Error occurs in any service\n   \u2193\nService returns { success: false, error: string }\n   \u2193\nMain application logs error\n   \u2193\nSystem tray returns to IDLE state\n   \u2193\nUser can retry operation\n</code></pre>"},{"location":"development/architecture/#technology-stack","title":"Technology Stack","text":""},{"location":"development/architecture/#runtime-build","title":"Runtime &amp; Build","text":"<ul> <li>Development: Bun \u22651.2.0 with TypeScript</li> <li>Production: Node.js \u226522 (npm distribution)</li> <li>Build: Bun bundler for single-file distribution</li> <li>Package Management: Bun for development, npm for distribution</li> </ul>"},{"location":"development/architecture/#core-dependencies","title":"Core Dependencies","text":"<pre><code>{\n  \"openai\": \"^5.11.0\",              // OpenAI API integration\n  \"node-systray-v2\": \"...\",         // System tray (improved fork)\n  \"clipboardy\": \"^4.0.0\"            // Cross-platform clipboard\n}\n</code></pre>"},{"location":"development/architecture/#system-requirements","title":"System Requirements","text":"<ul> <li>Linux: Ubuntu 22.04+ with alsa-utils and xsel</li> <li>Audio: ALSA-compatible sound system</li> <li>Desktop: System tray support (GNOME, KDE, XFCE)</li> </ul>"},{"location":"development/architecture/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"development/architecture/#memory-usage","title":"Memory Usage","text":"<ul> <li>Base: ~50MB (Node.js runtime + dependencies)</li> <li>Recording: +10MB (audio buffer)</li> <li>Processing: +20MB (API requests/responses)</li> </ul>"},{"location":"development/architecture/#api-usage","title":"API Usage","text":"<ul> <li>Whisper: ~$0.006 per minute of audio</li> <li>GPT-3.5-turbo: ~$0.002 per transcription formatting</li> <li>Rate Limits: Respects OpenAI API limits (no built-in retry)</li> </ul>"},{"location":"development/architecture/#file-system","title":"File System","text":"<ul> <li>Temp Files: Created in <code>/tmp/transcriber/</code></li> <li>Config: Stored in <code>~/.config/voice-transcriber/</code></li> <li>Cleanup: Automatic temp file cleanup on process exit</li> </ul>"},{"location":"development/architecture/#security-considerations","title":"Security Considerations","text":""},{"location":"development/architecture/#api-key-management","title":"API Key Management","text":"<ul> <li>Config file permissions: 600 (user read/write only)</li> <li>API key validation on startup</li> <li>No API key logging or exposure</li> </ul>"},{"location":"development/architecture/#audio-privacy","title":"Audio Privacy","text":"<ul> <li>Local audio processing only</li> <li>Temporary files cleaned up automatically</li> <li>No persistent audio storage</li> </ul>"},{"location":"development/architecture/#system-integration","title":"System Integration","text":"<ul> <li>Minimal system permissions required</li> <li>No elevated privileges needed</li> <li>Sandboxed execution environment</li> </ul>"},{"location":"development/architecture/#testing-strategy","title":"Testing Strategy","text":""},{"location":"development/architecture/#test-coverage","title":"Test Coverage","text":"<ul> <li>Unit Tests: 37 tests across all services</li> <li>Integration Tests: Full workflow validation</li> <li>Mock Strategy: Simple mocks for external dependencies</li> </ul>"},{"location":"development/architecture/#test-categories","title":"Test Categories","text":"<pre><code>// Service Tests\nAudioRecorder.test.ts     // Recording lifecycle\nTranscriptionService.test.ts  // API integration\nSystemTrayService.test.ts // UI state management\nConfig.test.ts           // Configuration handling\n\n// Integration Tests\nindex.test.ts            // Full application workflow\n</code></pre>"},{"location":"development/architecture/#testing-commands","title":"Testing Commands","text":"<pre><code>make test              # Run all tests\nmake test-watch        # Watch mode for development\nmake test-file FILE=   # Run specific test file\n</code></pre>"},{"location":"development/architecture/#development-workflow","title":"Development Workflow","text":""},{"location":"development/architecture/#setup","title":"Setup","text":"<pre><code>git clone &lt;repository&gt;\ncd voice-transcriber\nmake install           # Install dependencies\nmake check-deps        # Verify system requirements\ncp config.example.json config.json  # Setup config\n</code></pre>"},{"location":"development/architecture/#development-loop","title":"Development Loop","text":"<pre><code>make dev              # Start with auto-reload\nmake test             # Run tests\nmake format-check     # Lint and format\n</code></pre>"},{"location":"development/architecture/#build-release","title":"Build &amp; Release","text":"<pre><code>make build            # Build for production\nmake release-patch    # Create patch release\nnpm publish           # Publish to npm\n</code></pre>"},{"location":"development/architecture/#future-architecture-considerations","title":"Future Architecture Considerations","text":""},{"location":"development/architecture/#scalability","title":"Scalability","text":"<ul> <li>Plugin system for additional AI providers</li> <li>Configurable audio backends (PulseAudio, JACK)</li> <li>Multi-language prompt templates</li> </ul>"},{"location":"development/architecture/#platform-expansion","title":"Platform Expansion","text":"<ul> <li>Windows support (replace arecord with Windows Audio API)</li> <li>macOS support (replace arecord with Core Audio)</li> <li>Web version using WebRTC</li> </ul>"},{"location":"development/architecture/#performance-optimization","title":"Performance Optimization","text":"<ul> <li>Local Whisper model integration (faster-whisper)</li> <li>Audio compression before API upload</li> <li>Streaming transcription for long recordings</li> </ul>"},{"location":"development/architecture/#enhanced-features","title":"Enhanced Features","text":"<ul> <li>Keyboard shortcut integration (when Wayland supports it)</li> <li>Multiple output formats (Markdown, structured text)</li> <li>Batch processing capabilities</li> </ul>"},{"location":"development/development-guide/","title":"Voice Transcriber - Development Guide","text":""},{"location":"development/development-guide/#quick-start","title":"Quick Start","text":""},{"location":"development/development-guide/#prerequisites","title":"Prerequisites","text":"<p>System Requirements: <pre><code># Ubuntu/Debian\nsudo apt-get update\nsudo apt-get install alsa-utils xsel curl\n\n# Verify installations\nwhich arecord  # Should return /usr/bin/arecord\nwhich xsel     # Should return /usr/bin/xsel\n</code></pre></p> <p>Runtime Requirements: - Bun: \u22651.2.0 (development) - Node.js: \u226522 (production/npm) - OpenAI API Key: From https://platform.openai.com/api-keys</p>"},{"location":"development/development-guide/#initial-setup","title":"Initial Setup","text":"<pre><code># Clone repository\ngit clone https://github.com/Nouuu/voice-transcriber.git\ncd voice-transcriber\n\n# Install dependencies\nmake install\n\n# Verify system dependencies\nmake check-deps\n\n# Setup configuration\ncp config.example.json config.json\n# Edit config.json and add your OpenAI API key\n</code></pre>"},{"location":"development/development-guide/#first-run","title":"First Run","text":"<pre><code># Run tests to verify setup\nmake test\n\n# Start development mode\nmake dev\n\n# The app will prompt for API key if not configured\n# Look for green system tray icon\n</code></pre>"},{"location":"development/development-guide/#development-workflow","title":"Development Workflow","text":""},{"location":"development/development-guide/#daily-development-loop","title":"Daily Development Loop","text":"<ol> <li> <p>Start Development Mode <pre><code>make dev  # Auto-reload on file changes\n</code></pre></p> </li> <li> <p>Code Changes</p> </li> <li>Edit TypeScript files in <code>src/</code></li> <li>Bun automatically recompiles and restarts</li> <li> <p>Check console for errors/logs</p> </li> <li> <p>Test Changes <pre><code># Run all tests\nmake test\n\n# Run specific test file\nmake test-file FILE=src/services/system-tray.test.ts\n\n# Watch mode for TDD\nmake test-watch\n</code></pre></p> </li> <li> <p>Code Quality <pre><code># Check formatting and linting\nmake format-check\n\n# Fix formatting issues\nmake format\n</code></pre></p> </li> <li> <p>Build &amp; Verify <pre><code># Build for production\nmake build\n\n# Test production build\nnode dist/index.js\n</code></pre></p> </li> </ol>"},{"location":"development/development-guide/#project-structure","title":"Project Structure","text":"<pre><code>voice-transcriber/\n\u251c\u2500\u2500 src/                    # Source code\n\u2502   \u251c\u2500\u2500 index.ts           # Main application entry\n\u2502   \u251c\u2500\u2500 config/            # Configuration management\n\u2502   \u2502   \u251c\u2500\u2500 config.ts\n\u2502   \u2502   \u2514\u2500\u2500 config.test.ts\n\u2502   \u251c\u2500\u2500 services/          # Core services\n\u2502   \u2502   \u251c\u2500\u2500 audio-recorder.ts\n\u2502   \u2502   \u251c\u2500\u2500 system-tray.ts\n\u2502   \u2502   \u251c\u2500\u2500 transcription.ts\n\u2502   \u2502   \u251c\u2500\u2500 formatter.ts\n\u2502   \u2502   \u251c\u2500\u2500 clipboard.ts\n\u2502   \u2502   \u2514\u2500\u2500 *.test.ts      # Unit tests\n\u2502   \u2514\u2500\u2500 utils/             # Utilities\n\u2502       \u251c\u2500\u2500 logger.ts\n\u2502       \u2514\u2500\u2500 logger.test.ts\n\u251c\u2500\u2500 docs/                  # Documentation\n\u2502   \u251c\u2500\u2500 ARCHITECTURE.md\n\u2502   \u251c\u2500\u2500 API_REFERENCE.md\n\u2502   \u2514\u2500\u2500 DEVELOPMENT_GUIDE.md\n\u251c\u2500\u2500 assets/                # Icons and resources\n\u2502   \u251c\u2500\u2500 icon-idle.png\n\u2502   \u251c\u2500\u2500 icon-recording.png\n\u2502   \u2514\u2500\u2500 icon-processing.png\n\u251c\u2500\u2500 dist/                  # Built application\n\u251c\u2500\u2500 Makefile              # Development commands\n\u2514\u2500\u2500 package.json          # Dependencies and scripts\n</code></pre>"},{"location":"development/development-guide/#available-make-commands","title":"Available Make Commands","text":""},{"location":"development/development-guide/#core-commands","title":"Core Commands","text":"<pre><code>make help          # Show all available commands\nmake install       # Install bun dependencies\nmake run          # Run the application\nmake dev          # Run in development mode with watch\nmake build        # Build for production\nmake clean        # Clean build artifacts and temporary files\n</code></pre>"},{"location":"development/development-guide/#testing-commands","title":"Testing Commands","text":"<pre><code>make test         # Run all tests with bun test\nmake test-watch   # Run tests in watch mode\nmake test-file    # Run specific test (usage: make test-file FILE=path/to/test.ts)\n</code></pre>"},{"location":"development/development-guide/#code-quality-commands","title":"Code Quality Commands","text":"<pre><code>make lint         # Run ESLint linting\nmake format       # Format code with Prettier\nmake format-check # Check code formatting and linting\nmake check-deps   # Check system dependencies\n</code></pre>"},{"location":"development/development-guide/#release-commands","title":"Release Commands","text":"<pre><code>make release-patch  # Create patch release (x.x.X) - Bug fixes\nmake release-minor  # Create minor release (x.X.0) - New features\nmake release-major  # Create major release (X.0.0) - Breaking changes\nmake get-version   # Show current version from package.json\nmake pre-release   # Validate code before release\n</code></pre>"},{"location":"development/development-guide/#development-patterns","title":"Development Patterns","text":""},{"location":"development/development-guide/#service-development-pattern","title":"Service Development Pattern","text":"<p>All services follow a consistent pattern:</p> <pre><code>// 1. Define interfaces\nexport interface ServiceConfig {\n  // Configuration options\n}\n\nexport interface ServiceResult {\n  success: boolean;\n  error?: string;\n  // Additional result data\n}\n\n// 2. Implement service class\nexport class MyService {\n  private config: ServiceConfig;\n\n  constructor(config: ServiceConfig) {\n    this.config = config;\n  }\n\n  public async doSomething(): Promise&lt;ServiceResult&gt; {\n    try {\n      // Implementation logic\n      return { success: true };\n    } catch (error) {\n      return {\n        success: false,\n        error: `Operation failed: ${error}`\n      };\n    }\n  }\n}\n</code></pre>"},{"location":"development/development-guide/#error-handling-pattern","title":"Error Handling Pattern","text":"<pre><code>// Services never throw - always return result objects\nconst result = await service.doSomething();\n\nif (!result.success) {\n  logger.error(`Service failed: ${result.error}`);\n  // Handle error appropriately\n  return;\n}\n\n// Use result data\nconsole.log('Success:', result.data);\n</code></pre>"},{"location":"development/development-guide/#testing-pattern","title":"Testing Pattern","text":"<pre><code>import { describe, test, expect } from \"bun:test\";\nimport { MyService } from \"./my-service\";\n\ndescribe(\"MyService\", () =&gt; {\n  test(\"should handle success case\", async () =&gt; {\n    const service = new MyService({ /* config */ });\n    const result = await service.doSomething();\n\n    expect(result.success).toBe(true);\n    expect(result.error).toBeUndefined();\n  });\n\n  test(\"should handle error case\", async () =&gt; {\n    const service = new MyService({ /* invalid config */ });\n    const result = await service.doSomething();\n\n    expect(result.success).toBe(false);\n    expect(result.error).toContain(\"expected error message\");\n  });\n});\n</code></pre>"},{"location":"development/development-guide/#code-quality-guidelines","title":"Code Quality Guidelines","text":""},{"location":"development/development-guide/#simplicity-principles","title":"Simplicity Principles","text":"<p>\u2705 What to Implement: - Basic error handling (try/catch, return success/error) - Simple configuration loading from JSON - Direct API calls to OpenAI (Whisper + GPT) - Basic audio recording (start/stop/save) - Simple system tray with 3 states - Console logging (info/error only)</p> <p>\u274c What to Avoid: - Complex retry logic with exponential backoff - Advanced statistics tracking and usage metrics - Batch processing capabilities - Complex validation with detailed error messages - Advanced logging with rotation and file management</p>"},{"location":"development/development-guide/#service-guidelines","title":"Service Guidelines","text":"<ul> <li>Size: Keep services under 100 lines each</li> <li>Methods: 3-5 core methods maximum per service</li> <li>Interfaces: Use simple <code>{ success: boolean, error?: string }</code> pattern</li> <li>Dependencies: Minimize external dependencies</li> <li>Single Responsibility: Each service has one clear purpose</li> </ul>"},{"location":"development/development-guide/#typescript-guidelines","title":"TypeScript Guidelines","text":"<pre><code>// Use explicit interfaces\ninterface MyConfig {\n  apiKey: string;\n  enabled: boolean;\n}\n\n// Use type-safe result objects\ninterface MyResult {\n  success: boolean;\n  data?: MyData;\n  error?: string;\n}\n\n// Use async/await consistently\npublic async doWork(): Promise&lt;MyResult&gt; {\n  // Implementation\n}\n\n// Use proper error handling\ntry {\n  const result = await externalService();\n  return { success: true, data: result };\n} catch (error) {\n  return { success: false, error: `Failed: ${error}` };\n}\n</code></pre>"},{"location":"development/development-guide/#file-organization","title":"File Organization","text":"<pre><code>// File: src/services/my-service.ts\n\n// 1. Imports\nimport { external } from \"external-package\";\nimport { internal } from \"../utils/internal\";\n\n// 2. Interfaces\nexport interface MyConfig { }\nexport interface MyResult { }\n\n// 3. Implementation\nexport class MyService {\n  // Private fields first\n  private config: MyConfig;\n\n  // Constructor\n  constructor(config: MyConfig) { }\n\n  // Public methods\n  public async method(): Promise&lt;MyResult&gt; { }\n\n  // Private methods last\n  private helper(): void { }\n}\n</code></pre>"},{"location":"development/development-guide/#testing-strategy","title":"Testing Strategy","text":""},{"location":"development/development-guide/#test-organization","title":"Test Organization","text":"<pre><code>src/services/\n\u251c\u2500\u2500 audio-recorder.ts\n\u251c\u2500\u2500 audio-recorder.test.ts    # Unit tests for audio recorder\n\u251c\u2500\u2500 system-tray.ts\n\u251c\u2500\u2500 system-tray.test.ts       # Unit tests for system tray\n\u2514\u2500\u2500 ...\n</code></pre>"},{"location":"development/development-guide/#testing-commands_1","title":"Testing Commands","text":"<pre><code># Run all tests (37 tests total)\nmake test\n\n# Run specific service tests\nmake test-file FILE=src/services/system-tray.test.ts\n\n# Watch mode for TDD\nmake test-watch\n\n# Test with coverage (manual analysis)\nbun test --coverage\n</code></pre>"},{"location":"development/development-guide/#test-categories","title":"Test Categories","text":"<ol> <li>Unit Tests: Individual service functionality</li> <li>Integration Tests: Service interaction (index.test.ts)</li> <li>Mock Tests: External API simulation</li> <li>Error Tests: Error handling validation</li> </ol>"},{"location":"development/development-guide/#testing-best-practices","title":"Testing Best Practices","text":"<pre><code>describe(\"Service Tests\", () =&gt; {\n  test(\"success case - primary functionality\", async () =&gt; {\n    // Test main use case\n  });\n\n  test(\"error case - invalid input\", async () =&gt; {\n    // Test error handling\n  });\n\n  test(\"edge case - boundary conditions\", async () =&gt; {\n    // Test edge cases only if critical\n  });\n});\n\n// Keep tests simple and focused\n// Maximum 5-6 tests per service\n// Test behavior, not implementation\n</code></pre>"},{"location":"development/development-guide/#mock-strategy","title":"Mock Strategy","text":"<pre><code>// Simple mocks for external dependencies\nconst mockOpenAI = {\n  audio: {\n    transcriptions: {\n      create: async () =&gt; ({ text: \"mock transcription\" })\n    }\n  }\n};\n\n// Inject mocks via constructor\nconst service = new TranscriptionService(config, mockOpenAI);\n</code></pre>"},{"location":"development/development-guide/#debugging","title":"Debugging","text":""},{"location":"development/development-guide/#common-development-issues","title":"Common Development Issues","text":"<p>Application Won't Start: <pre><code># Check system dependencies\nmake check-deps\n\n# Verify API key in config\ncat config.json | grep apiKey\n\n# Check Bun installation\nbun --version\n</code></pre></p> <p>System Tray Not Visible: <pre><code># Test different desktop environments\necho $XDG_CURRENT_DESKTOP\n\n# Check system tray support\nps aux | grep -i tray\n\n# Try manual icon update\n# Restart window manager if needed\n</code></pre></p> <p>Audio Recording Issues: <pre><code># Test arecord directly\narecord -l  # List audio devices\narecord -D default -f cd -t wav test.wav  # Test recording\n\n# Check ALSA configuration\ncat /proc/asound/cards\n</code></pre></p> <p>API Issues: <pre><code># Test API key manually\ncurl -H \"Authorization: Bearer YOUR_API_KEY\" \\\n  https://api.openai.com/v1/models\n\n# Check rate limits\n# Monitor API usage in OpenAI dashboard\n</code></pre></p>"},{"location":"development/development-guide/#logging-and-diagnostics","title":"Logging and Diagnostics","text":"<p>Console Output: <pre><code># Development mode shows all logs\nmake dev\n\n# Production mode - minimal logging\nmake run\n</code></pre></p> <p>Debug Mode: <pre><code>// Add temporary debug logging\nconsole.log(\"Debug:\", { variable, state });\n\n// Use logger for permanent logging\nlogger.info(\"Operation completed\");\nlogger.error(\"Operation failed\");\n</code></pre></p> <p>File System Debugging: <pre><code># Check temp files\nls -la /tmp/transcriber/\n\n# Check config file\ncat ~/.config/voice-transcriber/config.json\n\n# Monitor file creation\nwatch -n 1 'ls -la /tmp/transcriber/'\n</code></pre></p>"},{"location":"development/development-guide/#building-and-distribution","title":"Building and Distribution","text":""},{"location":"development/development-guide/#development-build","title":"Development Build","text":"<pre><code># Build for testing\nmake build\n\n# Output: dist/index.js (single file)\n# Test build\nnode dist/index.js\n</code></pre>"},{"location":"development/development-guide/#production-release-process","title":"Production Release Process","text":"<pre><code># 1. Validate code quality\nmake pre-release\n\n# 2. Update version and create git tag\nmake release-patch  # or release-minor, release-major\n\n# 3. Verify build\nmake build\nmake test\n\n# 4. Publish to npm (manual step)\nnpm publish\n</code></pre>"},{"location":"development/development-guide/#build-configuration","title":"Build Configuration","text":"<pre><code>// bun build configuration (package.json)\n{\n  \"scripts\": {\n    \"build\": \"bun build src/index.ts --outdir dist --target node --format esm\"\n  }\n}\n</code></pre>"},{"location":"development/development-guide/#asset-handling","title":"Asset Handling","text":"<ul> <li>Icons: Embedded as Base64 in TypeScript files</li> <li>Config: Template provided as <code>config.example.json</code></li> <li>Dependencies: Bundled in single output file</li> </ul>"},{"location":"development/development-guide/#performance-optimization","title":"Performance Optimization","text":""},{"location":"development/development-guide/#development-performance","title":"Development Performance","text":"<pre><code># Fast restart in development\nmake dev  # Uses --watch flag for instant reload\n\n# Fast testing\nmake test-file FILE=specific-test.ts\n\n# Parallel testing (automatic with bun)\nbun test  # Runs tests in parallel by default\n</code></pre>"},{"location":"development/development-guide/#production-performance","title":"Production Performance","text":"<ul> <li>Bundle Size: ~2MB single file (including dependencies)</li> <li>Memory Usage: ~50MB base + 30MB during processing</li> <li>Startup Time: &lt;1 second on modern hardware</li> <li>API Latency: Depends on OpenAI response times</li> </ul>"},{"location":"development/development-guide/#optimization-tips","title":"Optimization Tips","text":"<ol> <li>Minimize API Calls: Batch operations when possible</li> <li>Audio Compression: Consider implementing before API upload</li> <li>Caching: Cache successful configurations</li> <li>Error Recovery: Implement graceful degradation</li> </ol>"},{"location":"development/development-guide/#contributing-guidelines","title":"Contributing Guidelines","text":""},{"location":"development/development-guide/#git-workflow","title":"Git Workflow","text":"<pre><code># Create feature branch\ngit checkout -b feature/my-feature\n\n# Make changes and commit\ngit add .\ngit commit -m \"feat: add new feature\"\n\n# Keep commits small and focused\n# Use conventional commit messages\n</code></pre>"},{"location":"development/development-guide/#commit-message-format","title":"Commit Message Format","text":"<pre><code># Format: type: description\nfeat: add system tray icon updates\nfix: resolve menu actions not working\nrefactor: simplify clipboard service\ntest: add system tray state tests\ndocs: update development guide\nchore: update dependencies\n\n# Keep under 50 characters\n# Use present tense\n# No capitalization after colon\n# No period at end\n</code></pre>"},{"location":"development/development-guide/#code-review-checklist","title":"Code Review Checklist","text":"<ul> <li> Follows simplicity principles</li> <li> Has unit tests for new functionality</li> <li> Uses consistent error handling pattern</li> <li> Maintains under 100 lines per service</li> <li> Includes appropriate logging</li> <li> Updates documentation if needed</li> <li> Passes all existing tests</li> </ul>"},{"location":"development/development-guide/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Create Feature Branch: <code>git checkout -b feature/description</code></li> <li>Implement Changes: Follow development patterns</li> <li>Add Tests: Ensure new functionality is tested</li> <li>Update Docs: If API or behavior changes</li> <li>Run Quality Checks: <code>make format-check &amp;&amp; make test</code></li> <li>Create PR: With clear description and testing notes</li> <li>Code Review: Address feedback and iterate</li> <li>Merge: Squash commits for clean history</li> </ol>"},{"location":"development/development-guide/#advanced-topics","title":"Advanced Topics","text":""},{"location":"development/development-guide/#custom-icon-development","title":"Custom Icon Development","text":"<pre><code># Create new icons (128x128 PNG)\n# Convert to Base64\nbase64 -w 0 icon.png &gt; icon.base64\n\n# Update src/services/system-tray.ts\n# Add to iconsBase64 object\n</code></pre>"},{"location":"development/development-guide/#api-integration-testing","title":"API Integration Testing","text":"<pre><code>// Create integration test with real API\nconst integrationTest = process.env.TEST_WITH_REAL_API;\n\nif (integrationTest) {\n  // Test with real OpenAI API\n  // Use test API key with low limits\n} else {\n  // Use mocks for regular testing\n}\n</code></pre>"},{"location":"development/development-guide/#performance-profiling","title":"Performance Profiling","text":"<pre><code># Profile memory usage\nbun --heap-usage src/index.ts\n\n# Profile startup time\ntime bun run src/index.ts\n\n# Monitor system resources\nhtop  # CPU and memory usage\niotop # Disk I/O usage\n</code></pre>"},{"location":"development/development-guide/#cross-platform-development","title":"Cross-Platform Development","text":"<pre><code>// Platform detection\nimport { platform } from \"node:os\";\n\nswitch (platform()) {\n  case 'linux':\n    // Linux-specific code (arecord)\n    break;\n  case 'win32':\n    // Windows-specific code (future)\n    break;\n  case 'darwin':\n    // macOS-specific code (future)\n    break;\n}\n</code></pre> <p>This development guide provides comprehensive information for contributing to and extending the Voice Transcriber application. Follow these patterns and practices to maintain code quality and consistency.</p>"},{"location":"development/testing/","title":"Voice Transcriber - Testing Guide","text":""},{"location":"development/testing/#testing-overview","title":"Testing Overview","text":"<p>The Voice Transcriber application uses Bun's built-in test runner with a comprehensive test suite covering all services. The testing strategy emphasizes simplicity, reliability, and maintainability following the project's core principles.</p>"},{"location":"development/testing/#test-strategy","title":"Test Strategy","text":""},{"location":"development/testing/#testing-philosophy","title":"Testing Philosophy","text":"<ul> <li>Keep It Simple: Test core functionality, not edge cases</li> <li>Focus on Behavior: Test what the service does, not how it does it</li> <li>Simple Mocks: Use straightforward mocks, avoid complex scenarios</li> <li>Maximum Coverage: 5-6 tests per service focusing on critical paths</li> <li>Fast Execution: Tests should run quickly for TDD workflow</li> </ul>"},{"location":"development/testing/#test-categories","title":"Test Categories","text":"<ol> <li>Unit Tests: Individual service functionality</li> <li>Integration Tests: Service interaction and full workflow</li> <li>Error Handling Tests: Validation of error scenarios</li> <li>Mock Tests: External dependency simulation</li> </ol>"},{"location":"development/testing/#test-structure","title":"Test Structure","text":""},{"location":"development/testing/#current-test-coverage","title":"Current Test Coverage","text":"<pre><code>Total Tests: 37 tests across all services\n\u251c\u2500\u2500 src/config/config.test.ts          (6 tests)\n\u251c\u2500\u2500 src/services/audio-recorder.test.ts (5 tests)\n\u251c\u2500\u2500 src/services/clipboard.test.ts      (5 tests)\n\u251c\u2500\u2500 src/services/formatter.test.ts      (6 tests)\n\u251c\u2500\u2500 src/services/system-tray.test.ts    (6 tests)\n\u251c\u2500\u2500 src/services/transcription.test.ts  (3 tests)\n\u251c\u2500\u2500 src/utils/logger.test.ts            (3 tests)\n\u2514\u2500\u2500 src/index.test.ts                   (3 tests)\n</code></pre>"},{"location":"development/testing/#test-organization-pattern","title":"Test Organization Pattern","text":"<p>Each service follows a consistent test structure:</p> <pre><code>import { beforeEach, describe, expect, it, mock } from \"bun:test\";\nimport { ServiceClass } from \"./service-name\";\n\ndescribe(\"ServiceClass\", () =&gt; {\n  let service: ServiceClass;\n  let config: ServiceConfig;\n\n  beforeEach(() =&gt; {\n    // Reset mocks\n    // Setup test configuration\n    // Initialize service\n  });\n\n  describe(\"methodName\", () =&gt; {\n    it(\"should handle success case\", async () =&gt; {\n      // Test primary functionality\n    });\n\n    it(\"should handle error case\", async () =&gt; {\n      // Test error scenarios\n    });\n\n    it(\"should validate input\", async () =&gt; {\n      // Test input validation\n    });\n  });\n});\n</code></pre>"},{"location":"development/testing/#running-tests","title":"Running Tests","text":""},{"location":"development/testing/#basic-test-commands","title":"Basic Test Commands","text":"<pre><code># Run all tests\nmake test\n\n# Run tests in watch mode (for TDD)\nmake test-watch\n\n# Run specific test file\nmake test-file FILE=src/services/system-tray.test.ts\n\n# Run tests with verbose output\nbun test --verbose\n\n# Run tests with coverage analysis\nbun test --coverage\n</code></pre>"},{"location":"development/testing/#test-output-interpretation","title":"Test Output Interpretation","text":"<pre><code># Successful test run example\n$ make test\n\u2713 Config should load default configuration\n\u2713 Config should handle missing config file\n\u2713 SystemTrayService should initialize successfully\n\u2713 TranscriptionService should handle API errors\n\n37 tests passed\n0 tests failed\n</code></pre> <p>Test Status Indicators: - \u2713 Test passed - \u2717 Test failed - \u23f8 Test skipped - \u26a0 Test had warnings</p>"},{"location":"development/testing/#test-examples","title":"Test Examples","text":""},{"location":"development/testing/#unit-test-example-systemtrayservice","title":"Unit Test Example - SystemTrayService","text":"<pre><code>// File: src/services/system-tray.test.ts\n\nimport { beforeEach, describe, expect, it, mock } from \"bun:test\";\n\n// Mock external dependency\nconst mockSystray = {\n  onClick: mock(),\n  onReady: mock(),\n  sendAction: mock(),\n  kill: mock(),\n};\n\nconst mockSysTrayConstructor = mock(() =&gt; mockSystray);\n\n// Mock the module before importing\nmock.module(\"node-systray-v2\", () =&gt; ({\n  SysTray: mockSysTrayConstructor,\n}));\n\nimport { SystemTrayService, TrayState } from \"./system-tray\";\n\ndescribe(\"SystemTrayService\", () =&gt; {\n  let service: SystemTrayService;\n  let config: TrayConfig;\n\n  beforeEach(() =&gt; {\n    // Reset all mocks\n    mockSysTrayConstructor.mockReset();\n    mockSystray.onClick.mockReset();\n\n    // Setup test configuration\n    config = {\n      callbacks: {\n        onRecordingStart: mock(),\n        onRecordingStop: mock(),\n        onQuit: mock(),\n      },\n    };\n\n    // Create service with mocked constructor\n    service = new SystemTrayService(config, mockSysTrayConstructor);\n  });\n\n  describe(\"initialize\", () =&gt; {\n    it(\"should initialize successfully\", async () =&gt; {\n      // Setup mock behavior\n      mockSysTrayConstructor.mockReturnValue(mockSystray);\n      mockSystray.onReady.mockImplementation(callback =&gt; callback());\n\n      // Execute test\n      const result = await service.initialize();\n\n      // Verify results\n      expect(result.success).toBe(true);\n      expect(mockSysTrayConstructor).toHaveBeenCalled();\n    });\n\n    it(\"should handle constructor errors\", async () =&gt; {\n      // Setup error scenario\n      mockSysTrayConstructor.mockImplementation(() =&gt; {\n        throw new Error(\"Mock constructor failed\");\n      });\n\n      // Execute test\n      const result = await service.initialize();\n\n      // Verify error handling\n      expect(result.success).toBe(false);\n      expect(result.error).toContain(\"Failed to initialize\");\n    });\n  });\n});\n</code></pre>"},{"location":"development/testing/#integration-test-example-main-application","title":"Integration Test Example - Main Application","text":"<pre><code>// File: src/index.test.ts\n\nimport { describe, expect, it, mock } from \"bun:test\";\nimport { VoiceTranscriberApp } from \"./index\";\n\ndescribe(\"VoiceTranscriberApp\", () =&gt; {\n  it(\"should fail to initialize without API key\", async () =&gt; {\n    const app = new VoiceTranscriberApp(\"/tmp/empty-config.json\");\n    const result = await app.initialize();\n\n    expect(result.success).toBe(false);\n    expect(result.error).toContain(\"API key not configured\");\n  });\n});\n</code></pre>"},{"location":"development/testing/#mock-strategy-example-transcriptionservice","title":"Mock Strategy Example - TranscriptionService","text":"<pre><code>// File: src/services/transcription.test.ts\n\nimport { beforeEach, describe, expect, it, mock } from \"bun:test\";\nimport { writeFileSync } from \"node:fs\";\nimport { TranscriptionService } from \"./transcription\";\n\n// Create simple mock for OpenAI\nconst mockOpenAI = {\n  audio: {\n    transcriptions: {\n      create: mock(),\n    },\n  },\n};\n\ndescribe(\"TranscriptionService\", () =&gt; {\n  let service: TranscriptionService;\n\n  beforeEach(() =&gt; {\n    // Reset mock between tests\n    mockOpenAI.audio.transcriptions.create.mockReset();\n\n    // Create service\n    service = new TranscriptionService({\n      apiKey: \"test-key\",\n    });\n\n    // Inject mock (bypassing private field)\n    (service as any).openai = mockOpenAI;\n  });\n\n  describe(\"transcribe\", () =&gt; {\n    it(\"should handle API errors\", async () =&gt; {\n      // Create temp test file\n      const tempFile = \"/tmp/test-transcription.txt\";\n      writeFileSync(tempFile, \"test audio content\");\n\n      // Setup mock to throw error\n      mockOpenAI.audio.transcriptions.create.mockRejectedValueOnce(\n        new Error(\"API Error\")\n      );\n\n      // Execute test\n      const result = await service.transcribe(tempFile);\n\n      // Verify error handling\n      expect(result.success).toBe(false);\n      expect(result.error).toContain(\"API Error\");\n    });\n  });\n});\n</code></pre>"},{"location":"development/testing/#testing-patterns","title":"Testing Patterns","text":""},{"location":"development/testing/#mock-setup-pattern","title":"Mock Setup Pattern","text":"<pre><code>// 1. Create mock objects\nconst mockDependency = {\n  method1: mock(),\n  method2: mock(),\n};\n\n// 2. Mock module before importing\nmock.module(\"external-library\", () =&gt; ({\n  ExternalClass: mock(() =&gt; mockDependency),\n}));\n\n// 3. Import after mocking\nimport { ServiceToTest } from \"./service\";\n\n// 4. Reset mocks in beforeEach\nbeforeEach(() =&gt; {\n  mockDependency.method1.mockReset();\n  mockDependency.method2.mockReset();\n});\n</code></pre>"},{"location":"development/testing/#error-testing-pattern","title":"Error Testing Pattern","text":"<pre><code>it(\"should handle service errors\", async () =&gt; {\n  // Setup error scenario\n  mockDependency.method.mockRejectedValueOnce(new Error(\"Test Error\"));\n\n  // Execute operation\n  const result = await service.doSomething();\n\n  // Verify error handling\n  expect(result.success).toBe(false);\n  expect(result.error).toContain(\"Test Error\");\n});\n</code></pre>"},{"location":"development/testing/#file-system-testing-pattern","title":"File System Testing Pattern","text":"<pre><code>it(\"should handle file operations\", async () =&gt; {\n  // Create temporary test file\n  const tempFile = \"/tmp/test-file.txt\";\n  writeFileSync(tempFile, \"test content\");\n\n  // Test file operation\n  const result = await service.processFile(tempFile);\n\n  // Verify results\n  expect(result.success).toBe(true);\n\n  // Cleanup handled by OS (/tmp)\n});\n</code></pre>"},{"location":"development/testing/#async-testing-pattern","title":"Async Testing Pattern","text":"<pre><code>it(\"should handle async operations\", async () =&gt; {\n  // Setup async mock\n  mockDependency.asyncMethod.mockResolvedValueOnce({\n    data: \"test result\"\n  });\n\n  // Execute async operation\n  const result = await service.asyncOperation();\n\n  // Verify async result\n  expect(result.success).toBe(true);\n  expect(result.data).toBe(\"test result\");\n});\n</code></pre>"},{"location":"development/testing/#test-data-management","title":"Test Data Management","text":""},{"location":"development/testing/#configuration-testing","title":"Configuration Testing","text":"<pre><code>// Test configuration objects\nconst validConfig = {\n  apiKey: \"test-key\",\n  enabled: true\n};\n\nconst invalidConfig = {\n  apiKey: \"\", // Invalid empty key\n  enabled: true\n};\n\n// Use in tests\nit(\"should validate configuration\", () =&gt; {\n  expect(() =&gt; new Service(validConfig)).not.toThrow();\n  expect(() =&gt; new Service(invalidConfig)).toThrow();\n});\n</code></pre>"},{"location":"development/testing/#file-testing","title":"File Testing","text":"<pre><code>// Create test files in /tmp\nconst createTestFile = (content: string): string =&gt; {\n  const tempFile = `/tmp/test-${Date.now()}.txt`;\n  writeFileSync(tempFile, content);\n  return tempFile;\n};\n\n// Use in tests\nit(\"should process files\", async () =&gt; {\n  const testFile = createTestFile(\"test audio data\");\n  const result = await service.processFile(testFile);\n  expect(result.success).toBe(true);\n});\n</code></pre>"},{"location":"development/testing/#test-development-workflow","title":"Test Development Workflow","text":""},{"location":"development/testing/#test-driven-development-tdd","title":"Test-Driven Development (TDD)","text":"<pre><code># 1. Start test watcher\nmake test-watch\n\n# 2. Write failing test\nit(\"should do something new\", async () =&gt; {\n  const result = await service.newMethod();\n  expect(result.success).toBe(true);\n});\n\n# 3. Implement minimum code to pass\npublic async newMethod(): Promise&lt;ServiceResult&gt; {\n  return { success: true };\n}\n\n# 4. Refactor while keeping tests green\n# 5. Repeat cycle\n</code></pre>"},{"location":"development/testing/#adding-new-tests","title":"Adding New Tests","text":"<pre><code># 1. Create test file alongside source\ntouch src/services/new-service.test.ts\n\n# 2. Follow test structure pattern\n# 3. Run specific test during development\nmake test-file FILE=src/services/new-service.test.ts\n\n# 4. Run full suite to ensure no regressions\nmake test\n</code></pre>"},{"location":"development/testing/#debugging-test-failures","title":"Debugging Test Failures","text":"<pre><code>// Add debug logging to tests\nit(\"should debug test issue\", async () =&gt; {\n  console.log(\"Debug: service state\", service);\n\n  const result = await service.doSomething();\n\n  console.log(\"Debug: result\", result);\n  expect(result.success).toBe(true);\n});\n</code></pre> <pre><code># Run single test with verbose output\nbun test src/services/service.test.ts --verbose\n\n# Add temporary console.log statements\n# Remove debug code before committing\n</code></pre>"},{"location":"development/testing/#common-testing-issues","title":"Common Testing Issues","text":""},{"location":"development/testing/#mock-issues","title":"Mock Issues","text":"<p>Problem: Mock not working as expected <pre><code>// Wrong - mock after import\nimport { Service } from \"./service\";\nmock.module(\"dependency\", () =&gt; ({ Mock: mock() }));\n\n// Right - mock before import\nmock.module(\"dependency\", () =&gt; ({ Mock: mock() }));\nimport { Service } from \"./service\";\n</code></pre></p> <p>Solution: Always mock modules before importing the service under test.</p>"},{"location":"development/testing/#async-issues","title":"Async Issues","text":"<p>Problem: Test fails intermittently <pre><code>// Wrong - not awaiting async operation\nit(\"should handle async\", () =&gt; {\n  service.asyncMethod(); // Missing await\n  expect(mockDependency.method).toHaveBeenCalled();\n});\n\n// Right - properly await async operations\nit(\"should handle async\", async () =&gt; {\n  await service.asyncMethod();\n  expect(mockDependency.method).toHaveBeenCalled();\n});\n</code></pre></p>"},{"location":"development/testing/#file-system-issues","title":"File System Issues","text":"<p>Problem: Tests interfere with each other <pre><code>// Wrong - using same file names\nconst testFile = \"/tmp/test.txt\";\n\n// Right - unique file names\nconst testFile = `/tmp/test-${Date.now()}.txt`;\n</code></pre></p>"},{"location":"development/testing/#mock-reset-issues","title":"Mock Reset Issues","text":"<p>Problem: Mocks retain state between tests <pre><code>// Wrong - no reset between tests\ndescribe(\"Service\", () =&gt; {\n  it(\"test 1\", () =&gt; { /* uses mock */ });\n  it(\"test 2\", () =&gt; { /* mock still has state from test 1 */ });\n});\n\n// Right - reset in beforeEach\nbeforeEach(() =&gt; {\n  mockDependency.method.mockReset();\n});\n</code></pre></p>"},{"location":"development/testing/#performance-testing","title":"Performance Testing","text":""},{"location":"development/testing/#test-execution-speed","title":"Test Execution Speed","text":"<pre><code># Measure test execution time\ntime make test\n\n# Profile individual test files\ntime bun test src/services/system-tray.test.ts\n</code></pre>"},{"location":"development/testing/#memory-usage-testing","title":"Memory Usage Testing","text":"<pre><code>// Basic memory usage check\nit(\"should not leak memory\", async () =&gt; {\n  const initialMemory = process.memoryUsage().heapUsed;\n\n  // Perform operations\n  for (let i = 0; i &lt; 100; i++) {\n    await service.doSomething();\n  }\n\n  const finalMemory = process.memoryUsage().heapUsed;\n  const memoryIncrease = finalMemory - initialMemory;\n\n  // Should not increase significantly\n  expect(memoryIncrease).toBeLessThan(10 * 1024 * 1024); // 10MB\n});\n</code></pre>"},{"location":"development/testing/#test-maintenance","title":"Test Maintenance","text":""},{"location":"development/testing/#keeping-tests-updated","title":"Keeping Tests Updated","text":"<ol> <li>Update tests when APIs change</li> <li>Remove tests for deleted functionality</li> <li>Add tests for new features</li> <li>Refactor tests when refactoring code</li> </ol>"},{"location":"development/testing/#test-quality-checklist","title":"Test Quality Checklist","text":"<ul> <li> Tests are focused and test one thing</li> <li> Tests have descriptive names</li> <li> Tests use appropriate mocks</li> <li> Tests clean up after themselves</li> <li> Tests are deterministic (no random failures)</li> <li> Tests run quickly (&lt; 100ms each)</li> </ul>"},{"location":"development/testing/#common-maintenance-tasks","title":"Common Maintenance Tasks","text":"<pre><code># Update all test dependencies\nbun update\n\n# Check for unused test files\nfind src -name \"*.test.ts\" -exec grep -L \"describe\\|it\\|test\" {} \\;\n\n# Validate test naming conventions\ngrep -r \"it\\|test\" src --include=\"*.test.ts\" | grep -v \"should\"\n</code></pre> <p>This testing guide provides comprehensive information for understanding, writing, and maintaining tests in the Voice Transcriber application. Follow these patterns to ensure robust and maintainable test coverage.</p>"},{"location":"getting-started/configuration/","title":"Configuration Guide","text":""},{"location":"getting-started/configuration/#overview","title":"Overview","text":"<p>Voice Transcriber uses a simple JSON configuration file located at: <pre><code>~/.config/voice-transcriber/config.json\n</code></pre></p>"},{"location":"getting-started/configuration/#configuration-settings","title":"Configuration Settings","text":""},{"location":"getting-started/configuration/#required-settings","title":"Required Settings","text":""},{"location":"getting-started/configuration/#transcriptionbackend-string","title":"<code>transcription.backend</code> (string)","text":"<p>The transcription backend to use: <code>\"openai\"</code> or <code>\"speaches\"</code>.</p> <p>Default: <code>\"openai\"</code></p>"},{"location":"getting-started/configuration/#transcriptionopenaiapikey-string","title":"<code>transcription.openai.apiKey</code> (string)","text":"<p>Your OpenAI API key for accessing Whisper and GPT services (required when using OpenAI backend).</p> <p>How to get one: https://platform.openai.com/api-keys</p> <p>Example: <pre><code>{\n  \"transcription\": {\n    \"backend\": \"openai\",\n    \"openai\": {\n      \"apiKey\": \"sk-proj-abc123...\"\n    }\n  }\n}\n</code></pre></p>"},{"location":"getting-started/configuration/#optional-settings","title":"Optional Settings","text":""},{"location":"getting-started/configuration/#language-string","title":"<code>language</code> (string)","text":"<p>The primary language for transcription and formatting.</p> <p>Supported languages: - <code>\"fr\"</code> - French - <code>\"en\"</code> - English (default) - <code>\"es\"</code> - Spanish - <code>\"de\"</code> - German - <code>\"it\"</code> - Italian</p> <p>Default: <code>\"en\"</code></p> <p>How it works: - Whisper API uses this as the primary transcription language - A strong language-specific prompt prevents Whisper from switching languages mid-transcription - Formatter (GPT) maintains this language when formatting text</p> <p>Example: <pre><code>{\n  \"language\": \"fr\"\n}\n</code></pre></p>"},{"location":"getting-started/configuration/#formatterenabled-boolean","title":"<code>formatterEnabled</code> (boolean)","text":"<p>Enable or disable GPT text formatting after transcription.</p> <p>When enabled: Transcribed text is formatted with proper grammar and punctuation When disabled: Raw transcription is copied directly to clipboard</p> <p>Default: <code>true</code></p> <p>Example: <pre><code>{\n  \"formatterEnabled\": true\n}\n</code></pre></p>"},{"location":"getting-started/configuration/#transcriptionprompt-string-or-null","title":"<code>transcriptionPrompt</code> (string or null)","text":"<p>Custom prompt for Whisper transcription.</p> <p>Default: <code>null</code> (uses automatic language-specific prompt)</p> <p>When to use: Only if you need very specific transcription behavior that differs from the built-in prompts.</p> <p>Built-in prompt (when null): <pre><code>This is a [Language] audio recording. Transcribe the entire audio in [Language] only.\nDo NOT switch to English or translate. Keep all content in [Language], preserving\n[Language] sentence structure and grammar throughout the entire transcription.\n</code></pre></p> <p>Example: <pre><code>{\n  \"transcriptionPrompt\": \"Transcribe this technical French presentation, keeping technical English terms but French grammar.\"\n}\n</code></pre></p>"},{"location":"getting-started/configuration/#formattingprompt-string-or-null","title":"<code>formattingPrompt</code> (string or null)","text":"<p>Custom prompt for GPT text formatting.</p> <p>Default: <code>null</code> (uses automatic language-aware prompt)</p> <p>When to use: Only if you need specific formatting rules beyond grammar and punctuation.</p> <p>Built-in prompt (when null): <pre><code>Format this [Language] text with proper grammar, punctuation, and structure.\nKeep the text in [Language]. Do not translate to another language.\n</code></pre></p> <p>Example: <pre><code>{\n  \"formattingPrompt\": \"Format this French text in a formal business style with proper punctuation.\"\n}\n</code></pre></p>"},{"location":"getting-started/configuration/#benchmarkmode-boolean","title":"<code>benchmarkMode</code> (boolean)","text":"<p>NEW FEATURE - Compare OpenAI and Speaches side-by-side.</p> <p>Default: <code>false</code></p> <p>When enabled: - Transcribes audio with BOTH OpenAI Whisper and Speaches - Displays detailed comparison (speed, accuracy, text differences) - Automatically selects best result based on similarity score - Requires <code>--debug</code> flag to see comparison output - Requires both backends configured (OpenAI API key AND Speaches URL)</p> <p>Example: <pre><code>{\n  \"benchmarkMode\": true,\n  \"transcription\": {\n    \"backend\": \"speaches\",\n    \"openai\": {\n      \"apiKey\": \"sk-...\",\n      \"model\": \"whisper-1\"\n    },\n    \"speaches\": {\n      \"url\": \"http://localhost:8000/v1\",\n      \"apiKey\": \"none\",\n      \"model\": \"Systran/faster-whisper-base\"\n    }\n  }\n}\n</code></pre></p> <p>Usage: <pre><code>voice-transcriber --debug\n# Records audio, then shows:\n# - OpenAI transcription time\n# - Speaches transcription time\n# - Speed comparison\n# - Text similarity percentage\n# - Differences analysis\n</code></pre></p> <p>Best Use Cases</p> <ul> <li>Testing different Speaches models</li> <li>Comparing cloud vs self-hosted performance</li> <li>Validating Speaches accuracy for your use case</li> <li>Optimizing your transcription setup</li> </ul>"},{"location":"getting-started/configuration/#transcription-object","title":"<code>transcription</code> (object)","text":"<p>Backend configuration for transcription service.</p> <p>Structure: <pre><code>{\n  \"transcription\": {\n    \"backend\": \"openai\" | \"speaches\",\n    \"openai\": {\n      \"apiKey\": \"string\",\n      \"model\": \"string\"\n    },\n    \"speaches\": {\n      \"url\": \"string\",\n      \"apiKey\": \"string\",\n      \"model\": \"string\"\n    }\n  }\n}\n</code></pre></p> <p>Fields:</p> <ul> <li><code>backend</code> (required): <code>\"openai\"</code> or <code>\"speaches\"</code> - which backend to use</li> <li><code>openai.apiKey</code> (required for OpenAI): Your OpenAI API key</li> <li><code>openai.model</code> (optional): Whisper model, default <code>\"whisper-1\"</code></li> <li><code>speaches.url</code> (required for Speaches): Speaches server URL</li> <li><code>speaches.apiKey</code> (optional): API key for Speaches, default <code>\"none\"</code></li> <li><code>speaches.model</code> (optional): Whisper model, default <code>\"Systran/faster-whisper-base\"</code></li> </ul> <p>Note: For <code>benchmarkMode</code>, both <code>openai</code> and <code>speaches</code> sections must be configured.</p>"},{"location":"getting-started/configuration/#complete-configuration-examples","title":"Complete Configuration Examples","text":""},{"location":"getting-started/configuration/#minimal-configuration-openai-backend","title":"Minimal Configuration (OpenAI Backend)","text":"<pre><code>{\n  \"language\": \"en\",\n  \"formatterEnabled\": true,\n  \"transcription\": {\n    \"backend\": \"openai\",\n    \"openai\": {\n      \"apiKey\": \"sk-proj-abc123...\"\n    }\n  }\n}\n</code></pre>"},{"location":"getting-started/configuration/#full-configuration-with-all-options","title":"Full Configuration with All Options","text":"<pre><code>{\n  \"language\": \"en\",\n  \"formatterEnabled\": true,\n  \"transcriptionPrompt\": null,\n  \"formattingPrompt\": null,\n  \"benchmarkMode\": false,\n  \"transcription\": {\n    \"backend\": \"openai\",\n    \"openai\": {\n      \"apiKey\": \"sk-proj-abc123...\",\n      \"model\": \"whisper-1\"\n    },\n    \"speaches\": {\n      \"url\": \"http://localhost:8000/v1\",\n      \"apiKey\": \"none\",\n      \"model\": \"Systran/faster-whisper-base\"\n    }\n  }\n}\n</code></pre>"},{"location":"getting-started/configuration/#speaches-backend-configuration","title":"Speaches Backend Configuration","text":"<pre><code>{\n  \"language\": \"fr\",\n  \"formatterEnabled\": false,\n  \"transcription\": {\n    \"backend\": \"speaches\",\n    \"speaches\": {\n      \"url\": \"http://localhost:8000/v1\",\n      \"apiKey\": \"none\",\n      \"model\": \"Systran/faster-whisper-base\"\n    }\n  }\n}\n</code></pre>"},{"location":"getting-started/configuration/#benchmark-mode-configuration","title":"Benchmark Mode Configuration","text":"<pre><code>{\n  \"language\": \"fr\",\n  \"formatterEnabled\": true,\n  \"benchmarkMode\": true,\n  \"transcription\": {\n    \"backend\": \"speaches\",\n    \"openai\": {\n      \"apiKey\": \"sk-proj-abc123...\",\n      \"model\": \"whisper-1\"\n    },\n    \"speaches\": {\n      \"url\": \"http://localhost:8000/v1\",\n      \"apiKey\": \"none\",\n      \"model\": \"Systran/faster-whisper-medium\"\n    }\n  }\n}\n</code></pre>"},{"location":"getting-started/configuration/#custom-prompts-configuration","title":"Custom Prompts Configuration","text":"<pre><code>{\n  \"language\": \"fr\",\n  \"formatterEnabled\": true,\n  \"transcriptionPrompt\": \"Transcribe this French audio with proper names and technical terms.\",\n  \"formattingPrompt\": \"Format this French text in a concise, professional style.\",\n  \"transcription\": {\n    \"backend\": \"openai\",\n    \"openai\": {\n      \"apiKey\": \"sk-proj-abc123...\"\n    }\n  }\n}\n</code></pre>"},{"location":"getting-started/configuration/#configuration-architecture","title":"Configuration Architecture","text":"<p>The configuration system uses a centralized, single-source-of-truth design:</p> <ol> <li>User Config (<code>~/.config/voice-transcriber/config.json</code>)</li> <li>Simple JSON file with core settings</li> <li> <p>Easy to understand and modify</p> </li> <li> <p>Config Class (<code>src/config/config.ts</code>)</p> </li> <li>Loads and validates user configuration</li> <li>Generates service-specific configurations</li> <li> <p>Provides strong language-specific prompts</p> </li> <li> <p>Service Configuration</p> </li> <li>Services receive fully-configured objects</li> <li>No configuration logic in services</li> <li>Services focus only on their core functionality</li> </ol>"},{"location":"getting-started/configuration/#flow-diagram","title":"Flow Diagram","text":"<pre><code>config.json\n    \u2193\nConfig.load()\n    \u2193\nConfig.getTranscriptionConfig() \u2192 TranscriptionService\nConfig.getFormatterConfig()     \u2192 FormatterService\n</code></pre>"},{"location":"getting-started/configuration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/configuration/#french-transcriptions-switching-to-english","title":"French Transcriptions Switching to English","text":"<p>Problem: Long French transcriptions switch to English mid-way.</p> <p>Solution: The new configuration system includes strong language-specific prompts that prevent this. Make sure: 1. Your config has <code>\"language\": \"fr\"</code> 2. You're not using a custom <code>transcriptionPrompt</code> (use <code>null</code> for default) 3. Restart the application after config changes</p> <p>How it works: - The <code>language</code> setting triggers a strong prompt like: \"This is a French audio recording. Transcribe the entire audio in French only. Do NOT switch to English...\"</p>"},{"location":"getting-started/configuration/#configuration-file-not-found","title":"Configuration File Not Found","text":"<p>Problem: Application can't find config file.</p> <p>Solution: Run the setup wizard: <pre><code>rm -rf ~/.config/voice-transcriber/config.json\nmake run  # Will trigger first-run setup\n</code></pre></p>"},{"location":"getting-started/configuration/#invalid-json-format","title":"Invalid JSON Format","text":"<p>Problem: Config file has syntax errors.</p> <p>Solution: Validate JSON: <pre><code>cat ~/.config/voice-transcriber/config.json | jq .\n</code></pre></p> <p>If invalid, use one of the complete examples above or recreate the file.</p>"},{"location":"getting-started/configuration/#next-steps","title":"Next Steps","text":"<ul> <li>First Run Guide - Complete setup walkthrough</li> <li>Transcription Backends - OpenAI vs Speaches</li> <li>Language Support - Multilingual transcription</li> </ul>"},{"location":"getting-started/first-run/","title":"First Run Guide","text":"<p>This guide walks you through your first experience with Voice Transcriber.</p>"},{"location":"getting-started/first-run/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have:</p> <ul> <li>\u2705 Installed Voice Transcriber</li> <li>\u2705 Obtained an OpenAI API key</li> <li>\u2705 Verified system dependencies with <code>make check-system-deps</code></li> </ul>"},{"location":"getting-started/first-run/#step-1-launch-the-application","title":"Step 1: Launch the Application","text":"<p>Start Voice Transcriber for the first time:</p> Global InstallationLocal Installation <pre><code>voice-transcriber\n</code></pre> <pre><code>cd voice-transcriber\nmake run\n</code></pre>"},{"location":"getting-started/first-run/#step-2-configuration-wizard","title":"Step 2: Configuration Wizard","text":"<p>If this is your first run, you'll see the Configuration Wizard:</p> <pre><code>\ud83c\udfa4 Voice Transcriber - First Run Setup\n\nNo configuration found. Let's set up your OpenAI API key.\n\n\ud83d\udcdd Get your API key at: https://platform.openai.com/api-keys\n\nEnter your OpenAI API key: _\n</code></pre> <p>Enter your API key when prompted. The wizard will:</p> <ol> <li>Validate your API key</li> <li>Create configuration file at <code>~/.config/voice-transcriber/config.json</code></li> <li>Set default language to English</li> <li>Enable text formatting by default</li> </ol> <p>Setup Complete</p> <p>Once configured, you'll see: \"\u2705 Configuration saved successfully!\"</p>"},{"location":"getting-started/first-run/#step-3-locate-the-system-tray-icon","title":"Step 3: Locate the System Tray Icon","text":"<p>Look for the green circle icon in your system tray:</p> <ul> <li> <p> IDLE (Green)</p> <p>Ready to record. Click to start recording.</p> </li> <li> <p> RECORDING (Red)</p> <p>Currently recording audio. Click to stop.</p> </li> <li> <p> PROCESSING (Purple)</p> <p>Transcribing audio. Wait for completion.</p> </li> </ul> <p>System Tray Location</p> <ul> <li>GNOME: Top-right corner (may need \"AppIndicator Support\" extension)</li> <li>KDE Plasma: Bottom-right panel</li> <li>XFCE/MATE: Top or bottom panel (near clock)</li> </ul>"},{"location":"getting-started/first-run/#step-4-your-first-recording","title":"Step 4: Your First Recording","text":""},{"location":"getting-started/first-run/#record-audio","title":"Record Audio","text":"<ol> <li>Click the system tray icon (or select \"Start Recording\" from menu)</li> <li>Icon changes to red circle \ud83d\udd34</li> <li>Speak into your microphone</li> <li>Click again to stop recording</li> </ol> <p>Example Recording</p> <p>\"Hello, this is my first test recording with Voice Transcriber.\"</p>"},{"location":"getting-started/first-run/#processing","title":"Processing","text":"<ol> <li>Icon changes to purple circle \ud83d\udfe3</li> <li>Audio is transcribed by OpenAI Whisper</li> <li>Text is formatted by GPT (if enabled)</li> <li>Result is copied to your clipboard automatically</li> </ol>"},{"location":"getting-started/first-run/#paste-result","title":"Paste Result","text":"<ol> <li>Open any text editor (e.g., gedit, VS Code, browser)</li> <li>Paste (Ctrl+V) the transcribed text</li> </ol> <p>Expected Result</p> <p>\"Hello, this is my first test recording with Voice Transcriber.\"</p>"},{"location":"getting-started/first-run/#step-5-test-different-languages-optional","title":"Step 5: Test Different Languages (Optional)","text":""},{"location":"getting-started/first-run/#configure-french","title":"Configure French","text":"<p>Edit your configuration:</p> <pre><code>nano ~/.config/voice-transcriber/config.json\n</code></pre> <p>Change language to French:</p> <pre><code>{\n  \"openaiApiKey\": \"sk-...\",\n  \"language\": \"fr\",\n  \"formatterEnabled\": true\n}\n</code></pre> <p>Restart the application and try a French recording:</p> <p>French Recording</p> <p>\"Bonjour, ceci est un test de transcription en fran\u00e7ais.\"</p> <p>Expected paste result: <pre><code>Bonjour, ceci est un test de transcription en fran\u00e7ais.\n</code></pre></p>"},{"location":"getting-started/first-run/#step-6-explore-menu-options","title":"Step 6: Explore Menu Options","text":"<p>Right-click the system tray icon to see available options:</p> <pre><code>\ud83c\udfa4 Voice Transcriber\n\u251c\u2500\u2500 \ud83c\udf99\ufe0f Start Recording\n\u251c\u2500\u2500 \u23f9\ufe0f Stop Recording\n\u251c\u2500\u2500 \u2699\ufe0f Settings\n\u2514\u2500\u2500 \u274c Quit\n</code></pre>"},{"location":"getting-started/first-run/#menu-actions","title":"Menu Actions","text":"<ul> <li>Start Recording: Begin audio capture</li> <li>Stop Recording: End recording and transcribe</li> <li>Settings: Opens config file in default text editor</li> <li>Quit: Exit the application gracefully</li> </ul>"},{"location":"getting-started/first-run/#common-first-run-issues","title":"Common First-Run Issues","text":""},{"location":"getting-started/first-run/#icon-not-visible","title":"Icon Not Visible","text":"<p>System Tray Not Showing</p> <p>GNOME Users: Install \"AppIndicator Support\" extension</p> <pre><code># Install extension\nsudo apt-get install gnome-shell-extension-appindicator\n\n# Restart GNOME Shell\n# Press Alt+F2, type 'r', press Enter\n</code></pre>"},{"location":"getting-started/first-run/#audio-recording-fails","title":"Audio Recording Fails","text":"<p>arecord: device not found</p> <p>Solution: Check audio devices</p> <pre><code># List audio devices\narecord -l\n\n# Test recording manually\narecord -d 5 test.wav\nplay test.wav\n</code></pre>"},{"location":"getting-started/first-run/#api-key-invalid","title":"API Key Invalid","text":"<p>OpenAI API Error: Invalid API key</p> <p>Solutions:</p> <ol> <li>Verify your API key at OpenAI Platform</li> <li>Check for extra spaces in config file</li> <li>Ensure key starts with <code>sk-</code></li> <li>Verify API key has Whisper API access</li> </ol>"},{"location":"getting-started/first-run/#transcription-in-wrong-language","title":"Transcription in Wrong Language","text":"<p>French recorded but English transcribed</p> <p>Solution: Set language explicitly in config</p> <pre><code>{\n  \"language\": \"fr\"\n}\n</code></pre> <p>Restart the application after config changes.</p>"},{"location":"getting-started/first-run/#debug-mode","title":"Debug Mode","text":"<p>Enable debug mode for detailed logging:</p> <pre><code># Global installation\nvoice-transcriber --debug\n\n# Local installation\nmake run ARGS=\"--debug\"\n</code></pre> <p>Debug output example:</p> <pre><code>2025-10-11T10:30:15.123Z [DEBUG] WAV file size: 2.45 MB\n2025-10-11T10:30:15.234Z [DEBUG] MP3 compression: 74.7% reduction\n2025-10-11T10:30:16.789Z [INFO] OpenAI transcription completed in 1.55s\n2025-10-11T10:30:16.789Z [DEBUG]   \u2514\u2500 Transcription: 142 characters\n</code></pre>"},{"location":"getting-started/first-run/#next-steps","title":"Next Steps","text":"<p>Now that you've completed your first recording, explore more features:</p> <ul> <li>Basic Usage - Learn all recording options</li> <li>Language Support - Multilingual transcription</li> <li>Configuration - Advanced settings and custom prompts</li> <li>Self-Hosted Setup - Run 100% offline</li> </ul>"},{"location":"getting-started/first-run/#quick-reference-card","title":"Quick Reference Card","text":""},{"location":"getting-started/first-run/#essential-commands","title":"Essential Commands","text":"<pre><code># Run application\nvoice-transcriber              # or: make run\n\n# Enable debug mode\nvoice-transcriber --debug\n\n# Edit configuration\nnano ~/.config/voice-transcriber/config.json\n\n# Check system dependencies\nmake check-system-deps\n\n# Test audio devices\narecord -l\n</code></pre>"},{"location":"getting-started/first-run/#recording-workflow","title":"Recording Workflow","text":"<pre><code>1. Click tray icon \u2192 \ud83d\udfe2 to \ud83d\udd34\n2. Speak into microphone\n3. Click again \u2192 \ud83d\udd34 to \ud83d\udfe3\n4. Wait for processing\n5. Paste result (Ctrl+V)\n</code></pre> <p>Need Help?</p> <ul> <li>Troubleshooting Guide</li> <li>GitHub Issues</li> </ul>"},{"location":"getting-started/installation/","title":"Installation Guide","text":""},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<p>Before installing Voice Transcriber, ensure you have the following:</p>"},{"location":"getting-started/installation/#1-bun-runtime-120","title":"1. Bun Runtime (\u22651.2.0)","text":"<p>Install Bun runtime for development and local usage:</p> <pre><code>curl -fsSL https://bun.sh/install | bash\n</code></pre> <p>Verify installation: <pre><code>bun --version\n# Should output: 1.2.0 or higher\n</code></pre></p>"},{"location":"getting-started/installation/#2-system-dependencies-ubuntulinux","title":"2. System Dependencies (Ubuntu/Linux)","text":"<p>Install required system packages:</p> <pre><code>sudo apt-get update\nsudo apt-get install alsa-utils xsel\n</code></pre> <p>Package purposes:</p> <ul> <li><code>alsa-utils</code>: Provides <code>arecord</code> for audio recording</li> <li><code>xsel</code>: Cross-platform clipboard integration</li> </ul> <p>Verify installation:</p> <pre><code>which arecord  # Should output: /usr/bin/arecord\nwhich xsel     # Should output: /usr/bin/xsel\n</code></pre>"},{"location":"getting-started/installation/#installation-methods","title":"Installation Methods","text":""},{"location":"getting-started/installation/#automated-setup-recommended","title":"Automated Setup (Recommended)","text":"<p>The fastest way to get started:</p> <pre><code># Step 1: Clone the repository\ngit clone https://github.com/Nouuu/voice-transcriber.git\ncd voice-transcriber\n\n# Step 2: Run automated setup\nmake setup\n</code></pre> <p>This command will:</p> <ul> <li>\u2705 Check all system dependencies (Bun, arecord, xsel)</li> <li>\u2705 Install Bun dependencies</li> <li>\u2705 Create configuration file at <code>~/.config/voice-transcriber/config.json</code></li> </ul>"},{"location":"getting-started/installation/#manual-setup","title":"Manual Setup","text":"<p>If you prefer step-by-step installation:</p> <pre><code># Step 1: Clone repository\ngit clone https://github.com/Nouuu/voice-transcriber.git\ncd voice-transcriber\n\n# Step 2: Check system dependencies\nmake check-system-deps\n\n# Step 3: Install Bun dependencies\nmake install\n\n# Step 4: Initialize configuration file\nmake init-config\n</code></pre>"},{"location":"getting-started/installation/#configuration","title":"Configuration","text":"<p>After installation, configure your OpenAI API key:</p> <pre><code># Edit configuration file\nnano ~/.config/voice-transcriber/config.json\n</code></pre> <p>Add your OpenAI API key:</p> <pre><code>{\n  \"openaiApiKey\": \"sk-your-api-key-here\",\n  \"language\": \"en\",\n  \"formatterEnabled\": true\n}\n</code></pre> <p>Get your OpenAI API key: https://platform.openai.com/api-keys</p> <p>Configuration Options</p> <p>See the Configuration Guide for detailed configuration options including language settings, custom prompts, and self-hosted backends.</p>"},{"location":"getting-started/installation/#global-installation-optional","title":"Global Installation (Optional)","text":"<p>Install the <code>voice-transcriber</code> command globally:</p> <pre><code>make install-global\n</code></pre> <p>This allows you to run the application from anywhere:</p> <pre><code># Run from any directory\nvoice-transcriber\n\n# Enable debug mode\nvoice-transcriber --debug\n</code></pre>"},{"location":"getting-started/installation/#uninstall-global-command","title":"Uninstall Global Command","text":"<p>To remove the global command:</p> <pre><code>make uninstall-global\n</code></pre>"},{"location":"getting-started/installation/#running-the-application","title":"Running the Application","text":""},{"location":"getting-started/installation/#development-mode","title":"Development Mode","text":"<p>Run with auto-reload for development:</p> <pre><code>make dev\n</code></pre>"},{"location":"getting-started/installation/#production-mode","title":"Production Mode","text":"<p>Run the application normally:</p> <pre><code># If installed globally\nvoice-transcriber\n\n# Or from project directory\nmake run\n</code></pre>"},{"location":"getting-started/installation/#debug-mode","title":"Debug Mode","text":"<p>Enable detailed logging for troubleshooting:</p> <pre><code># Global installation\nvoice-transcriber --debug\n\n# Or from project directory\nmake run ARGS=\"--debug\"\n</code></pre> <p>Debug output includes:</p> <ul> <li>File sizes (WAV and MP3 compression ratios)</li> <li>Audio format details (sample rate, channels)</li> <li>Processing times (upload, processing, response)</li> <li>Transcription metrics (character count, duration)</li> </ul>"},{"location":"getting-started/installation/#verification","title":"Verification","text":"<p>Verify your installation:</p> <pre><code># 1. Check system dependencies\nmake check-system-deps\n\n# 2. Verify configuration\ncat ~/.config/voice-transcriber/config.json\n\n# 3. Test audio recording\narecord -l  # Should list your audio devices\n\n# 4. Run the application\nmake run\n</code></pre> <p>Look for the green circle system tray icon indicating the app is ready.</p>"},{"location":"getting-started/installation/#platform-specific-notes","title":"Platform-Specific Notes","text":""},{"location":"getting-started/installation/#ubuntu-2204","title":"Ubuntu 22.04+","text":"<p>Fully supported with default installation steps.</p>"},{"location":"getting-started/installation/#other-linux-distributions","title":"Other Linux Distributions","text":"<ul> <li>Debian/Mint: Same as Ubuntu instructions</li> <li>Arch/Manjaro: Use <code>pacman -S alsa-utils xsel</code></li> <li>Fedora/RHEL: Use <code>dnf install alsa-utils xsel</code></li> </ul>"},{"location":"getting-started/installation/#wayland-vs-x11","title":"Wayland vs X11","text":"<p>Works on both Wayland and X11 desktop environments. System tray integration tested on:</p> <ul> <li>GNOME (Wayland and X11)</li> <li>KDE Plasma</li> <li>XFCE</li> <li>MATE</li> </ul> <p>Windows and macOS Support</p> <p>Currently, Voice Transcriber only supports Linux. Windows and macOS support is planned for future releases.</p>"},{"location":"getting-started/installation/#troubleshooting-installation","title":"Troubleshooting Installation","text":""},{"location":"getting-started/installation/#bun-installation-fails","title":"Bun Installation Fails","text":"<p>If Bun installation fails:</p> <pre><code># Try alternative installation method\nnpm install -g bun\n\n# Or download binary directly\ncurl -fsSL https://bun.sh/install | bash -s \"bun-v1.2.0\"\n</code></pre>"},{"location":"getting-started/installation/#system-dependencies-not-found","title":"System Dependencies Not Found","text":"<p>If <code>arecord</code> or <code>xsel</code> are not found:</p> <pre><code># Verify package manager\nwhich apt-get  # Debian/Ubuntu\nwhich pacman   # Arch\nwhich dnf      # Fedora\n\n# Update package cache\nsudo apt-get update  # Ubuntu\n</code></pre>"},{"location":"getting-started/installation/#permission-issues","title":"Permission Issues","text":"<p>If you encounter permission errors:</p> <pre><code># Ensure user is in audio group\nsudo usermod -a -G audio $USER\n\n# Log out and log back in for changes to take effect\n</code></pre>"},{"location":"getting-started/installation/#config-file-not-created","title":"Config File Not Created","text":"<p>If config file isn't created automatically:</p> <pre><code># Create directory manually\nmkdir -p ~/.config/voice-transcriber\n\n# Initialize config\nmake init-config\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration Guide - Configure languages and backends</li> <li>First Run - First-time setup walkthrough</li> <li>Basic Usage - Learn how to use the app</li> </ul> <p>Need Help?</p> <ul> <li>Troubleshooting Guide - Common issues and solutions</li> <li>GitHub Issues - Report bugs or request features</li> </ul>"},{"location":"user-guide/basic-usage/","title":"Basic Usage Guide","text":"<p>Learn how to use Voice Transcriber effectively for everyday voice-to-text transcription.</p>"},{"location":"user-guide/basic-usage/#system-tray-interface","title":"System Tray Interface","text":"<p>Voice Transcriber operates through a system tray icon with three visual states:</p> <ul> <li> <p> IDLE</p> <p>Green circle - Ready to record</p> <p>Click to start recording</p> </li> <li> <p> RECORDING</p> <p>Red circle - Currently recording</p> <p>Speak into microphone, click to stop</p> </li> <li> <p> PROCESSING</p> <p>Purple circle - Transcribing audio</p> <p>Wait for completion, result copied to clipboard</p> </li> </ul>"},{"location":"user-guide/basic-usage/#recording-audio","title":"Recording Audio","text":""},{"location":"user-guide/basic-usage/#start-recording","title":"Start Recording","text":"<p>Method 1: Click tray icon</p> <ol> <li>Left-click the system tray icon (green circle)</li> <li>Icon changes to red</li> <li>Start speaking into your microphone</li> </ol> <p>Method 2: Context menu</p> <ol> <li>Right-click the system tray icon</li> <li>Select \"Start Recording\"</li> <li>Icon changes to red</li> <li>Start speaking</li> </ol> <p>Microphone Position</p> <p>For best results, position your microphone 15-30 cm from your mouth</p>"},{"location":"user-guide/basic-usage/#stop-recording","title":"Stop Recording","text":"<p>Method 1: Click tray icon again</p> <ol> <li>Left-click the system tray icon (red circle)</li> <li>Icon changes to purple (processing)</li> <li>Wait for transcription</li> </ol> <p>Method 2: Context menu</p> <ol> <li>Right-click the system tray icon</li> <li>Select \"Stop Recording\"</li> <li>Icon changes to purple (processing)</li> </ol>"},{"location":"user-guide/basic-usage/#automatic-clipboard-copy","title":"Automatic Clipboard Copy","text":"<p>Once processing completes:</p> <ol> <li>Icon returns to green (idle)</li> <li>Transcribed text is automatically copied to your clipboard</li> <li>Paste anywhere with <code>Ctrl+V</code> (Linux) or <code>Cmd+V</code> (macOS)</li> </ol>"},{"location":"user-guide/basic-usage/#example-workflow","title":"Example Workflow","text":"Quick NoteEmail DictationMeeting Notes <pre><code>1. Click tray icon (\ud83d\udfe2 \u2192 \ud83d\udd34)\n2. \"Make a note to follow up with the client tomorrow\"\n3. Click again (\ud83d\udd34 \u2192 \ud83d\udfe3)\n4. Wait 2-3 seconds\n5. Paste in your notes app: \"Make a note to follow up with the client tomorrow.\"\n</code></pre> <pre><code>1. Click tray icon\n2. \"Hi team, I wanted to share an update on the project...\"\n3. Click again\n4. Wait for processing\n5. Paste in email client: \"Hi team, I wanted to share an update on the project...\"\n</code></pre> <pre><code>1. Start recording at beginning of meeting\n2. Let meeting proceed naturally\n3. Stop recording at end\n4. Paste transcription in notes document\n5. Review and edit as needed\n</code></pre>"},{"location":"user-guide/basic-usage/#context-menu-options","title":"Context Menu Options","text":"<p>Right-click the tray icon for additional actions:</p> <pre><code>\ud83c\udfa4 Voice Transcriber\n\u251c\u2500\u2500 \ud83c\udf99\ufe0f Start Recording\n\u251c\u2500\u2500 \u23f9\ufe0f Stop Recording\n\u251c\u2500\u2500 \u2699\ufe0f Settings\n\u2514\u2500\u2500 \u274c Quit\n</code></pre>"},{"location":"user-guide/basic-usage/#menu-actions","title":"Menu Actions","text":"Start Recording Begin audio capture (same as left-click when idle) Stop Recording End recording and transcribe (same as left-click when recording) Settings Opens <code>~/.config/voice-transcriber/config.json</code> in your default text editor Quit Exit the application gracefully"},{"location":"user-guide/basic-usage/#debug-mode","title":"Debug Mode","text":"<p>Enable debug mode for detailed logging and performance metrics:</p> <pre><code>voice-transcriber --debug\n</code></pre> <p>Debug output includes:</p> <ul> <li>File sizes: WAV and MP3 compression ratios</li> <li>Audio format: Sample rate, channels, conversion details</li> <li>Processing times: Upload, processing, and response breakdown</li> <li>Transcription metrics: Character count and duration</li> </ul> <p>Example debug output:</p> <pre><code>2025-10-11T10:30:15.123Z [DEBUG] WAV file size: 2.45 MB (2569216 bytes)\n2025-10-11T10:30:15.125Z [DEBUG] WAV format: 2 channel(s), 44100 Hz sample rate\n2025-10-11T10:30:15.234Z [DEBUG] MP3 file size: 0.62 MB (650240 bytes)\n2025-10-11T10:30:15.234Z [DEBUG] Compression ratio: 74.7% size reduction\n2025-10-11T10:30:16.789Z [INFO] OpenAI transcription completed in 1.55s\n2025-10-11T10:30:16.789Z [DEBUG]   \u2514\u2500 Estimated breakdown: upload ~0.47s, processing ~0.93s, receive ~0.16s\n</code></pre>"},{"location":"user-guide/basic-usage/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/basic-usage/#for-accuracy","title":"For Accuracy","text":"<p>Improve Transcription Quality</p> <ul> <li>Speak clearly at a moderate pace</li> <li>Minimize background noise when possible</li> <li>Use a quality microphone for best results</li> <li>Pause between sentences for better formatting</li> </ul>"},{"location":"user-guide/basic-usage/#for-efficiency","title":"For Efficiency","text":"<p>Maximize Productivity</p> <ul> <li>Use keyboard shortcuts (if your desktop environment supports global hotkeys)</li> <li>Record in chunks for long dictations (easier to review)</li> <li>Review transcriptions before using (especially for technical content)</li> <li>Customize language settings for consistent results</li> </ul>"},{"location":"user-guide/basic-usage/#for-multilingual-use","title":"For Multilingual Use","text":"<p>Language Consistency</p> <ul> <li>Set primary language in config file</li> <li>Restart after language changes for prompts to take effect</li> <li>Avoid language mixing for best accuracy</li> <li>See Language Support for details</li> </ul>"},{"location":"user-guide/basic-usage/#common-use-cases","title":"Common Use Cases","text":""},{"location":"user-guide/basic-usage/#note-taking","title":"Note Taking","text":"<p>Perfect for: Meeting notes, lecture notes, brainstorming sessions</p> <p>Tips: - Record entire meeting or lecture - Transcribe in segments for easier review - Edit transcription afterward for clarity</p>"},{"location":"user-guide/basic-usage/#message-dictation","title":"Message Dictation","text":"<p>Perfect for: Emails, chat messages, social media posts</p> <p>Tips: - Speak naturally but with clear punctuation - Enable formatter for proper capitalization - Review before sending</p>"},{"location":"user-guide/basic-usage/#content-creation","title":"Content Creation","text":"<p>Perfect for: Blog posts, articles, scripts</p> <p>Tips: - Outline first, then dictate sections - Use short recordings for easier editing - Transcribe ideas quickly without typing</p>"},{"location":"user-guide/basic-usage/#accessibility","title":"Accessibility","text":"<p>Perfect for: Users with typing difficulties, RSI, or mobility issues</p> <p>Tips: - Configure comfortable microphone position - Use voice commands in conjunction with transcription - Combine with accessibility tools in your OS</p>"},{"location":"user-guide/basic-usage/#performance-expectations","title":"Performance Expectations","text":""},{"location":"user-guide/basic-usage/#transcription-speed","title":"Transcription Speed","text":"Recording Length Processing Time Notes 5-10 seconds 1-2 seconds Near-instant 30 seconds 2-4 seconds Very fast 1 minute 3-6 seconds Fast 5 minutes 10-20 seconds Moderate 10+ minutes 20-40 seconds Longer wait <p>Processing Time Factors</p> <ul> <li>OpenAI API response time</li> <li>Audio file size and compression</li> <li>Internet connection speed</li> <li>GPT formatting (adds 1-2 seconds if enabled)</li> </ul>"},{"location":"user-guide/basic-usage/#accuracy-expectations","title":"Accuracy Expectations","text":"Content Type Expected Accuracy Notes Clear speech 95-98% Excellent Technical terms 85-95% Good, may need review Accented speech 80-95% Varies by accent Noisy environment 70-85% Reduced accuracy Mixed languages 75-90% See language support"},{"location":"user-guide/basic-usage/#keyboard-shortcuts-future-feature","title":"Keyboard Shortcuts (Future Feature)","text":"<p>Coming Soon</p> <p>Global keyboard shortcuts are planned for a future release. Currently, Wayland security restrictions prevent global hotkey registration. This feature will be available when Wayland adds proper hotkey support.</p>"},{"location":"user-guide/basic-usage/#next-steps","title":"Next Steps","text":"<ul> <li>Language Support - Multilingual transcription</li> <li>Transcription Backends - OpenAI vs Speaches</li> <li>Troubleshooting - Common issues and solutions</li> </ul> <p>Need Help?</p> <ul> <li>Troubleshooting Guide</li> <li>GitHub Issues</li> </ul>"},{"location":"user-guide/language-support/","title":"Language Support","text":"<p>Voice Transcriber supports multiple languages with strong language enforcement to prevent unwanted language switching during transcription.</p>"},{"location":"user-guide/language-support/#supported-languages","title":"Supported Languages","text":"Language Code Quality Notes English <code>en</code> \u2b50\u2b50\u2b50\u2b50\u2b50 Default, excellent accuracy French <code>fr</code> \u2b50\u2b50\u2b50\u2b50\u2b50 Excellent, strong enforcement Spanish <code>es</code> \u2b50\u2b50\u2b50\u2b50\u2b50 Excellent support German <code>de</code> \u2b50\u2b50\u2b50\u2b50\u2b50 Excellent support Italian <code>it</code> \u2b50\u2b50\u2b50\u2b50\u2b50 Excellent support"},{"location":"user-guide/language-support/#configuring-language","title":"Configuring Language","text":"<p>Edit your configuration file:</p> <pre><code>nano ~/.config/voice-transcriber/config.json\n</code></pre> <p>Set the <code>language</code> field:</p> <pre><code>{\n  \"language\": \"fr\",\n  \"formatterEnabled\": true,\n  \"transcription\": {\n    \"backend\": \"openai\",\n    \"openai\": {\n      \"apiKey\": \"sk-...\"\n    }\n  }\n}\n</code></pre> <p>Restart the application for changes to take effect.</p>"},{"location":"user-guide/language-support/#language-enforcement","title":"Language Enforcement","text":"<p>Voice Transcriber uses strong language-specific prompts to prevent Whisper from switching languages mid-transcription.</p>"},{"location":"user-guide/language-support/#how-it-works","title":"How It Works","text":"<p>When you set a language (e.g., French):</p> <ol> <li> <p>Transcription prompt explicitly instructs Whisper:    <pre><code>This is a French audio recording. Transcribe the entire audio in French only.\nDo NOT switch to English or translate. Keep all content in French.\n</code></pre></p> </li> <li> <p>Formatting prompt maintains the language:    <pre><code>Format this French text with proper grammar and punctuation.\nKeep the text in French. Do not translate.\n</code></pre></p> </li> </ol> <p>This dual-layer enforcement ensures your transcriptions stay in the configured language.</p>"},{"location":"user-guide/language-support/#examples-by-language","title":"Examples by Language","text":""},{"location":"user-guide/language-support/#english","title":"English","text":"<p>Recording: \"Hello, this is a test of the transcription system\"</p> <p>Result: \"Hello, this is a test of the transcription system.\"</p>"},{"location":"user-guide/language-support/#french","title":"French","text":"<p>Recording: \"Bonjour, ceci est un test du syst\u00e8me de transcription\"</p> <p>Result: \"Bonjour, ceci est un test du syst\u00e8me de transcription.\"</p>"},{"location":"user-guide/language-support/#spanish","title":"Spanish","text":"<p>Recording: \"Hola, esta es una prueba del sistema de transcripci\u00f3n\"</p> <p>Result: \"Hola, esta es una prueba del sistema de transcripci\u00f3n.\"</p>"},{"location":"user-guide/language-support/#mixed-language-content","title":"Mixed-Language Content","text":"<p>For recordings with technical English terms in French sentences (common in tech contexts):</p> <p>Configuration: <pre><code>{\n  \"language\": \"fr\",\n  \"transcriptionPrompt\": \"Transcribe this French audio. Keep technical English terms but preserve French grammar.\",\n  \"transcription\": {\n    \"backend\": \"openai\",\n    \"openai\": {\n      \"apiKey\": \"sk-...\"\n    }\n  }\n}\n</code></pre></p> <p>Recording: \"J'utilise React avec TypeScript pour d\u00e9velopper l'application\"</p> <p>Result: \"J'utilise React avec TypeScript pour d\u00e9velopper l'application.\"</p> <p>Custom Prompts</p> <p>See Configuration Guide for advanced mixed-language scenarios.</p>"},{"location":"user-guide/language-support/#troubleshooting-language-issues","title":"Troubleshooting Language Issues","text":""},{"location":"user-guide/language-support/#french-transcription-switches-to-english","title":"French Transcription Switches to English","text":"<p>Problem: Long French recordings switch to English mid-way</p> <p>Solution: Ensure language is set explicitly:</p> <pre><code>{\n  \"language\": \"fr\",\n  \"transcription\": {\n    \"backend\": \"openai\",\n    \"openai\": {\n      \"apiKey\": \"sk-...\"\n    }\n  }\n}\n</code></pre> <p>Restart the application after config changes.</p>"},{"location":"user-guide/language-support/#technical-terms-not-recognized","title":"Technical Terms Not Recognized","text":"<p>Problem: Technical English terms in French speech are poorly transcribed</p> <p>Solution: Use custom transcription prompt:</p> <pre><code>{\n  \"language\": \"fr\",\n  \"transcriptionPrompt\": \"Transcribe this French technical audio. Keep common English technical terms as-is.\",\n  \"transcription\": {\n    \"backend\": \"openai\",\n    \"openai\": {\n      \"apiKey\": \"sk-...\"\n    }\n  }\n}\n</code></pre>"},{"location":"user-guide/language-support/#accent-not-recognized-well","title":"Accent Not Recognized Well","text":"<p>Problem: Strong accent reduces accuracy</p> <p>Solutions: - Speak slightly slower and more clearly - Reduce background noise for better recognition - Try self-hosted Speaches with different models (see Speaches Integration)</p>"},{"location":"user-guide/language-support/#language-specific-tips","title":"Language-Specific Tips","text":""},{"location":"user-guide/language-support/#french_1","title":"French","text":"<ul> <li>Accents (\u00e9, \u00e8, \u00e0, \u00e7) are generally recognized well</li> <li>Homophones (ou/o\u00f9, a/\u00e0) may need manual correction</li> <li>Long compound sentences benefit from formatting</li> </ul>"},{"location":"user-guide/language-support/#spanish_1","title":"Spanish","text":"<ul> <li>Accents and \u00f1 are well recognized</li> <li>Regional variations (Spain vs Latin America) work well</li> <li>Inverted question marks may need manual addition</li> </ul>"},{"location":"user-guide/language-support/#german","title":"German","text":"<ul> <li>Umlauts (\u00e4, \u00f6, \u00fc, \u00df) are recognized accurately</li> <li>Compound words are usually transcribed correctly</li> <li>Formal/informal you (Sie/du) preserved correctly</li> </ul>"},{"location":"user-guide/language-support/#italian","title":"Italian","text":"<ul> <li>Accents (\u00e0, \u00e8, \u00ec, \u00f2, \u00f9) are well recognized</li> <li>Double consonants are usually correct</li> <li>Regional variations work well</li> </ul>"},{"location":"user-guide/language-support/#performance-by-language","title":"Performance by Language","text":"<p>All supported languages have similar processing times:</p> Language Avg Processing Time Accuracy English 1.5-2.5s per 30s 95-98% French 1.5-2.5s per 30s 93-97% Spanish 1.5-2.5s per 30s 93-97% German 1.5-2.5s per 30s 92-96% Italian 1.5-2.5s per 30s 93-97%"},{"location":"user-guide/language-support/#adding-more-languages","title":"Adding More Languages","text":"<p>While only 5 languages have strong enforcement prompts, Whisper supports many more:</p> <p>Other supported codes: <code>ja</code>, <code>zh</code>, <code>pt</code>, <code>ru</code>, <code>ko</code>, <code>ar</code>, <code>nl</code>, <code>pl</code>, <code>tr</code>, etc.</p> <p>To use unsupported language:</p> <pre><code>{\n  \"language\": \"pt\",\n  \"transcriptionPrompt\": \"Transcribe this Portuguese audio completely in Portuguese. Do not switch languages.\",\n  \"transcription\": {\n    \"backend\": \"openai\",\n    \"openai\": {\n      \"apiKey\": \"sk-...\"\n    }\n  }\n}\n</code></pre> <p>Community Contributions</p> <p>If you use Voice Transcriber in other languages and develop effective prompts, please contribute them via GitHub Pull Request!</p>"},{"location":"user-guide/language-support/#next-steps","title":"Next Steps","text":"<ul> <li>Basic Usage - Learn recording workflow</li> <li>Configuration Guide - Custom prompts and settings</li> <li>Transcription Backends - OpenAI vs Speaches</li> </ul> <p>Need Help?</p> <ul> <li>Troubleshooting Guide</li> <li>GitHub Issues</li> </ul>"},{"location":"user-guide/transcription-backends/","title":"Transcription Backends","text":"<p>Voice Transcriber supports two transcription backends: OpenAI Whisper (cloud) and Speaches (self-hosted).</p>"},{"location":"user-guide/transcription-backends/#backend-comparison","title":"Backend Comparison","text":"Feature OpenAI Whisper \u2601\ufe0f Speaches \ud83c\udfe0 Setup Zero setup Docker required Cost ~$0.006/minute Free (self-hosted) Privacy Audio sent to OpenAI 100% offline Speed Very fast (1.5-2.5s/30s) Comparable with base model Accuracy Excellent (95-98%) Excellent (91-100%) Internet Required Not required"},{"location":"user-guide/transcription-backends/#openai-whisper-cloud","title":"OpenAI Whisper (Cloud)","text":"<p>Best for: Quick setup, occasional use, no local resources</p>"},{"location":"user-guide/transcription-backends/#configuration","title":"Configuration","text":"<pre><code>{\n  \"openaiApiKey\": \"sk-your-api-key-here\",\n  \"language\": \"en\",\n  \"transcription\": {\n    \"backend\": \"openai\"\n  }\n}\n</code></pre>"},{"location":"user-guide/transcription-backends/#pros","title":"Pros","text":"<ul> <li>\u2705 Zero setup required</li> <li>\u2705 No local resources needed</li> <li>\u2705 Consistently fast processing</li> <li>\u2705 High accuracy across languages</li> </ul>"},{"location":"user-guide/transcription-backends/#cons","title":"Cons","text":"<ul> <li>\u274c Requires internet connection</li> <li>\u274c API costs ($0.006 per minute of audio)</li> <li>\u274c Audio data sent to OpenAI servers</li> <li>\u274c Subject to OpenAI API rate limits</li> </ul>"},{"location":"user-guide/transcription-backends/#speaches-self-hosted","title":"Speaches (Self-Hosted)","text":"<p>Best for: Privacy-conscious users, high-volume use, offline operation</p> <p>Powered by Speaches - OpenAI-compatible speech-to-text server</p>"},{"location":"user-guide/transcription-backends/#quick-setup-3-commands","title":"Quick Setup (3 commands)","text":"<pre><code># 1. Create docker-compose.speaches.yml\ndocker compose -f docker-compose.speaches.yml up -d\n\n# 2. Update config\nnano ~/.config/voice-transcriber/config.json\n# Change \"backend\": \"openai\" to \"backend\": \"speaches\"\n\n# 3. Done! First transcription downloads model (~140MB)\n</code></pre>"},{"location":"user-guide/transcription-backends/#configuration_1","title":"Configuration","text":"<pre><code>{\n  \"language\": \"fr\",\n  \"formatterEnabled\": false,\n  \"transcription\": {\n    \"backend\": \"speaches\",\n    \"speaches\": {\n      \"url\": \"http://localhost:8000/v1\",\n      \"apiKey\": \"none\",\n      \"model\": \"Systran/faster-whisper-base\"\n    }\n  }\n}\n</code></pre>"},{"location":"user-guide/transcription-backends/#pros_1","title":"Pros","text":"<ul> <li>\u2705 Zero cost - No API fees</li> <li>\u2705 Complete privacy - Audio never leaves your machine</li> <li>\u2705 Offline operation - No internet required after model download</li> <li>\u2705 Same speed - Base model comparable to OpenAI (3.7s vs 3.8s)</li> <li>\u2705 High accuracy - 91-100% similarity depending on model</li> </ul>"},{"location":"user-guide/transcription-backends/#cons_1","title":"Cons","text":"<ul> <li>\u274c Requires Docker setup</li> <li>\u274c Initial model download (~140MB-2.9GB depending on model)</li> <li>\u274c Requires local compute resources</li> <li>\u274c Larger models need more RAM/CPU</li> </ul>"},{"location":"user-guide/transcription-backends/#performance-comparison","title":"Performance Comparison","text":"<p>Benchmark: 30s French audio, Remote server (8 CPU / 8GB RAM)</p> Model OpenAI Speaches (CPU) Speed Ratio Accuracy tiny 1.98s 2.81s 0.70x 92.4% base \u2b50 3.70s 3.81s 0.97x 91.4% small 2.23s 7.15s 0.31x 97.4% medium 3.70s 25.82s 0.14x 96.1% large-v3 2.55s 30.80s 0.08x 100.0% <p>Recommendation: Base Model</p> <p>The base model offers the best balance: nearly identical speed to OpenAI, 91% accuracy, and zero cost.</p>"},{"location":"user-guide/transcription-backends/#choosing-a-backend","title":"Choosing a Backend","text":""},{"location":"user-guide/transcription-backends/#use-openai-whisper-if","title":"Use OpenAI Whisper if:","text":"<ul> <li>\ud83d\udcf1 You want zero setup and immediate use</li> <li>\ud83c\udf10 You always have internet connection</li> <li>\ud83d\udcb5 Cost is acceptable for your usage volume</li> <li>\ud83c\udfaf You prioritize convenience over privacy</li> </ul>"},{"location":"user-guide/transcription-backends/#use-speaches-if","title":"Use Speaches if:","text":"<ul> <li>\ud83d\udd12 Privacy is important (audio never leaves your machine)</li> <li>\ud83d\udcb0 You transcribe frequently (avoid API costs)</li> <li>\ud83d\udcf4 You need offline operation</li> <li>\ud83c\udfe0 You have local compute resources (or can spin up a VPS)</li> </ul>"},{"location":"user-guide/transcription-backends/#switching-backends","title":"Switching Backends","text":""},{"location":"user-guide/transcription-backends/#openai-speaches","title":"OpenAI \u2192 Speaches","text":"<pre><code># 1. Setup Speaches with Docker\ndocker compose -f docker-compose.speaches.yml up -d\n\n# 2. Update config\nnano ~/.config/voice-transcriber/config.json\n</code></pre> <p>Change: <pre><code>{\n  \"transcription\": {\n    \"backend\": \"speaches\"\n  }\n}\n</code></pre></p> <p>Restart the application.</p>"},{"location":"user-guide/transcription-backends/#speaches-openai","title":"Speaches \u2192 OpenAI","text":"<pre><code># Update config\nnano ~/.config/voice-transcriber/config.json\n</code></pre> <p>Change: <pre><code>{\n  \"openaiApiKey\": \"sk-...\",\n  \"transcription\": {\n    \"backend\": \"openai\"\n  }\n}\n</code></pre></p> <p>Restart the application.</p>"},{"location":"user-guide/transcription-backends/#benchmark-mode","title":"Benchmark Mode","text":"<p>Compare both backends side-by-side:</p> <pre><code>{\n  \"benchmarkMode\": true,\n  \"transcription\": {\n    \"backend\": \"speaches\",\n    \"openai\": {\n      \"apiKey\": \"sk-...\",\n      \"model\": \"whisper-1\"\n    },\n    \"speaches\": {\n      \"url\": \"http://localhost:8000/v1\",\n      \"model\": \"Systran/faster-whisper-base\"\n    }\n  }\n}\n</code></pre> <p>Run with debug mode: <pre><code>voice-transcriber --debug\n</code></pre></p> <p>Output example: <pre><code>\ud83d\udd2c BENCHMARK: Comparing OpenAI and Speaches\n\u23f1\ufe0f  Performance:\n   OpenAI Whisper:   2.45s\n   Speaches:         0.87s\n   Speedup:          2.82x faster\n\n\ud83d\udccf Text Length:\n   OpenAI:   142 chars\n   Speaches: 145 chars\n   Difference: 3 chars (2.1%)\n\n\ud83c\udfaf Similarity: 97.2% match\n</code></pre></p>"},{"location":"user-guide/transcription-backends/#next-steps","title":"Next Steps","text":"<ul> <li>Speaches Integration Guide - Detailed setup</li> <li>Whisper Models Comparison - Model selection</li> <li>Configuration Guide - Advanced settings</li> </ul> <p>Need Help?</p> <ul> <li>Troubleshooting Guide</li> <li>GitHub Issues</li> </ul>"},{"location":"user-guide/troubleshooting/","title":"Voice Transcriber - Troubleshooting Guide","text":""},{"location":"user-guide/troubleshooting/#quick-diagnostics","title":"Quick Diagnostics","text":""},{"location":"user-guide/troubleshooting/#first-steps-for-any-issue","title":"First Steps for Any Issue","text":"<pre><code># 1. Check system dependencies\nmake check-deps\n\n# 2. Verify configuration\ncat ~/.config/voice-transcriber/config.json\n\n# 3. Check recent logs\nmake dev  # Watch console output\n\n# 4. Test with minimal setup\nmake test\n</code></pre>"},{"location":"user-guide/troubleshooting/#health-check-commands","title":"Health Check Commands","text":"<pre><code># System Dependencies\nwhich arecord    # Should return /usr/bin/arecord\nwhich xsel       # Should return /usr/bin/xsel\narecord -l       # List audio devices\n\n# Application Status\nmake run         # Check startup logs\nls -la ~/.config/voice-transcriber/  # Check config directory\nls -la /tmp/transcriber/  # Check temp files\n</code></pre>"},{"location":"user-guide/troubleshooting/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"user-guide/troubleshooting/#application-wont-start","title":"\ud83d\udea8 Application Won't Start","text":""},{"location":"user-guide/troubleshooting/#symptom","title":"Symptom","text":"<pre><code>$ make run\nFailed to start: OpenAI API key not configured\n</code></pre> <p>Solution 1: Missing API Key <pre><code># Check configuration\ncat ~/.config/voice-transcriber/config.json\n\n# If file missing or empty, add API key:\n{\n  \"language\": \"en\",\n  \"formatterEnabled\": true,\n  \"transcription\": {\n    \"backend\": \"openai\",\n    \"openai\": {\n      \"apiKey\": \"sk-your-api-key-here\"\n    }\n  }\n}\n\n# Get API key from: https://platform.openai.com/api-keys\n</code></pre></p> <p>Solution 2: Invalid Configuration Format <pre><code># Validate JSON format\ncat ~/.config/voice-transcriber/config.json | jq .\n\n# If JSON is invalid, recreate:\nrm ~/.config/voice-transcriber/config.json\nmake run  # Will trigger setup wizard\n</code></pre></p> <p>Solution 3: Permission Issues <pre><code># Check directory permissions\nls -la ~/.config/voice-transcriber/\n\n# Fix permissions if needed\nchmod 755 ~/.config/voice-transcriber/\nchmod 600 ~/.config/voice-transcriber/config.json\n</code></pre></p>"},{"location":"user-guide/troubleshooting/#system-tray-not-visible","title":"\ud83d\udd07 System Tray Not Visible","text":""},{"location":"user-guide/troubleshooting/#symptom_1","title":"Symptom","text":"<p>App starts successfully but no tray icon appears.</p> <p>Solution 1: Desktop Environment Check <pre><code># Check desktop environment\necho $XDG_CURRENT_DESKTOP\n\n# Known working environments:\n# - GNOME (with gnome-shell-extension-appindicator)\n# - KDE Plasma\n# - XFCE with xfce4-indicator-plugin\n# - MATE\n</code></pre></p> <p>Solution 2: Install System Tray Support <pre><code># GNOME - Install AppIndicator extension\n# Via GNOME Extensions website or:\nsudo apt install gnome-shell-extension-appindicator\n\n# XFCE - Install indicator plugin\nsudo apt install xfce4-indicator-plugin\n\n# After installation, restart desktop session\n</code></pre></p> <p>Solution 3: Alternative Desktop Environments <pre><code># Some minimal window managers don't support system trays\n# Try running with stalonetray or similar:\nsudo apt install stalonetray\nstalonetray &amp;  # Run in background\nmake run       # Then start voice transcriber\n</code></pre></p> <p>Solution 4: Check Tray Area - Look in different corners of your screen - Try right-clicking in the system tray area - Check if icons are hidden in an overflow menu - Some desktop themes hide the system tray by default</p>"},{"location":"user-guide/troubleshooting/#audio-recording-issues","title":"\ud83c\udfa4 Audio Recording Issues","text":""},{"location":"user-guide/troubleshooting/#symptom_2","title":"Symptom","text":"<pre><code>Recording failed: Failed to start recording: spawn arecord ENOENT\n</code></pre> <p>Solution 1: Install ALSA Utils <pre><code># Install required audio tools\nsudo apt-get update\nsudo apt-get install alsa-utils\n\n# Verify installation\nwhich arecord  # Should return /usr/bin/arecord\narecord --version\n</code></pre></p> <p>Solution 2: Audio Device Configuration <pre><code># List available audio devices\narecord -l\n\n# Test recording manually\narecord -D default -f cd -t wav /tmp/test.wav\n# Speak for a few seconds, then Ctrl+C\n\n# Play back to verify\naplay /tmp/test.wav\n</code></pre></p> <p>Solution 3: Permission Issues <pre><code># Add user to audio group\nsudo usermod -a -G audio $USER\n\n# Logout and login again for changes to take effect\n# Or run: newgrp audio\n\n# Verify group membership\ngroups | grep audio\n</code></pre></p> <p>Solution 4: PulseAudio Configuration <pre><code># If using PulseAudio, check status\npulseaudio --check -v\n\n# Restart PulseAudio if needed\npulseaudio -k\npulseaudio --start\n\n# Test with PulseAudio-specific device\narecord -D pulse -f cd -t wav /tmp/test-pulse.wav\n</code></pre></p>"},{"location":"user-guide/troubleshooting/#symptom_3","title":"Symptom","text":"<p>Recording starts but no audio is captured (empty/silent files).</p> <p>Solution 1: Microphone Level <pre><code># Open audio mixer\nalsamixer\n\n# Navigate to microphone/capture controls\n# Press Tab to switch to capture view\n# Use arrow keys to adjust levels\n# Ensure microphone is not muted (no MM indicator)\n</code></pre></p> <p>Solution 2: Default Input Device <pre><code># Check default input device\ncat /proc/asound/cards\n\n# Set specific device in arecord command\n# Edit src/services/audio-recorder.ts if needed:\n# Change \"-D\", \"default\" to \"-D\", \"hw:1,0\" (for example)\n</code></pre></p>"},{"location":"user-guide/troubleshooting/#api-connection-issues","title":"\ud83c\udf10 API Connection Issues","text":""},{"location":"user-guide/troubleshooting/#symptom_4","title":"Symptom","text":"<pre><code>Transcription failed: Failed to transcribe audio: Request failed with status 401\n</code></pre> <p>Solution 1: Invalid API Key <pre><code># Test API key manually\ncurl -H \"Authorization: Bearer YOUR_API_KEY\" \\\n  https://api.openai.com/v1/models\n\n# Should return list of models, not error\n# If error, get new API key from: https://platform.openai.com/api-keys\n</code></pre></p> <p>Solution 2: Rate Limiting <pre><code># Check OpenAI usage dashboard\n# Visit: https://platform.openai.com/usage\n\n# Wait for rate limit reset or upgrade plan\n# Monitor usage to avoid limits\n</code></pre></p> <p>Solution 3: Network Issues <pre><code># Test connectivity\ncurl -I https://api.openai.com\n\n# Check proxy settings if behind corporate firewall\necho $HTTP_PROXY\necho $HTTPS_PROXY\n\n# Configure proxy in environment if needed\nexport HTTPS_PROXY=http://proxy.company.com:8080\n</code></pre></p>"},{"location":"user-guide/troubleshooting/#symptom_5","title":"Symptom","text":"<pre><code>Transcription failed: Network timeout\n</code></pre> <p>Solution: Network Configuration <pre><code># Increase timeout (if needed, modify transcription service)\n# Check network stability\nping -c 4 api.openai.com\n\n# Test with smaller audio files first\n# Large files may timeout on slow connections\n</code></pre></p>"},{"location":"user-guide/troubleshooting/#clipboard-issues","title":"\ud83d\udccb Clipboard Issues","text":""},{"location":"user-guide/troubleshooting/#symptom_6","title":"Symptom","text":"<pre><code>Clipboard failed: Failed to write to clipboard: xsel: command not found\n</code></pre> <p>Solution 1: Install Clipboard Tools <pre><code># Install xsel (preferred) or xclip\nsudo apt-get install xsel\n\n# Or alternative:\nsudo apt-get install xclip\n\n# Verify installation\nwhich xsel\n</code></pre></p> <p>Solution 2: Display Environment <pre><code># Ensure DISPLAY is set for X11\necho $DISPLAY  # Should show :0 or similar\n\n# For Wayland, some clipboard tools need special handling\n# Try wl-clipboard instead:\nsudo apt-get install wl-clipboard\n</code></pre></p> <p>Solution 3: Permission Issues <pre><code># Test clipboard manually\necho \"test\" | xsel -ib  # Copy to clipboard\nxsel -ob               # Paste from clipboard\n\n# Should echo \"test\" if working correctly\n</code></pre></p>"},{"location":"user-guide/troubleshooting/#system-performance-issues","title":"\ud83d\udd04 System Performance Issues","text":""},{"location":"user-guide/troubleshooting/#symptom_7","title":"Symptom","text":"<p>High memory usage or slow performance.</p> <p>Solution 1: Monitor Resource Usage <pre><code># Monitor while running\nhtop\n# Look for voice-transcriber process\n\n# Check memory usage\nps aux | grep voice-transcriber\n\n# Monitor disk I/O\niotop\n</code></pre></p> <p>Solution 2: Cleanup Temporary Files <pre><code># Clean temp directory\nrm -rf /tmp/transcriber/*\n\n# Set cleanup cron job if needed\ncrontab -e\n# Add: 0 2 * * * rm -rf /tmp/transcriber/*\n</code></pre></p> <p>Solution 3: Reduce Resource Usage <pre><code># Disable text formatting to reduce API calls\n# Edit ~/.config/voice-transcriber/config.json:\n{\n  \"openaiApiKey\": \"your-key\",\n  \"formatterEnabled\": false\n}\n</code></pre></p>"},{"location":"user-guide/troubleshooting/#development-issues","title":"\ud83d\udd27 Development Issues","text":""},{"location":"user-guide/troubleshooting/#symptom_8","title":"Symptom","text":"<pre><code>$ make test\nCommand not found: bun\n</code></pre> <p>Solution 1: Install Bun <pre><code># Install Bun runtime\ncurl -fsSL https://bun.sh/install | bash\n\n# Reload shell\nsource ~/.bashrc  # or restart terminal\n\n# Verify installation\nbun --version\n</code></pre></p> <p>Solution 2: Use Node.js Alternative <pre><code># If Bun unavailable, use Node.js\nnpm install\nnpm test\nnpm run dev\n</code></pre></p>"},{"location":"user-guide/troubleshooting/#symptom_9","title":"Symptom","text":"<pre><code>Tests failing with mock errors\n</code></pre> <p>Solution: Reset Development Environment <pre><code># Clean and reinstall\nmake clean\nrm -rf node_modules\nmake install\n\n# Run tests individually to isolate issues\nmake test-file FILE=src/services/system-tray.test.ts\n</code></pre></p>"},{"location":"user-guide/troubleshooting/#build-and-distribution-issues","title":"\ud83d\udce6 Build and Distribution Issues","text":""},{"location":"user-guide/troubleshooting/#symptom_10","title":"Symptom","text":"<pre><code>Build fails with missing dependencies\n</code></pre> <p>Solution 1: Dependency Issues <pre><code># Verify all dependencies installed\nbun install\n\n# Check for peer dependency warnings\nbun install --production=false\n\n# Update dependencies if needed\nbun update\n</code></pre></p> <p>Solution 2: Asset Loading Issues <pre><code># Verify assets are included in build\nls -la dist/\ncat dist/index.js | grep -i icon\n\n# Check asset paths in production\nnode dist/index.js\n</code></pre></p>"},{"location":"user-guide/troubleshooting/#advanced-troubleshooting","title":"Advanced Troubleshooting","text":""},{"location":"user-guide/troubleshooting/#debug-mode","title":"Debug Mode","text":"<p>Enable detailed logging for debugging:</p> <pre><code>// Temporary debug logging in services\nconsole.log(\"Debug: service state\", { variable, state });\n\n// Or modify logger to show debug messages\n// In src/utils/logger.ts, add debug level\n</code></pre>"},{"location":"user-guide/troubleshooting/#system-integration-testing","title":"System Integration Testing","text":"<pre><code># Test each component independently\n\n# 1. Test audio recording\narecord -D default -f cd -t wav /tmp/manual-test.wav\n# Speak for 5 seconds, then Ctrl+C\n\n# 2. Test API connection\ncurl -X POST https://api.openai.com/v1/audio/transcriptions \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -H \"Content-Type: multipart/form-data\" \\\n  -F file=\"@/tmp/manual-test.wav\" \\\n  -F model=\"whisper-1\"\n\n# 3. Test clipboard\necho \"test clipboard\" | xsel -ib\nxsel -ob  # Should output \"test clipboard\"\n</code></pre>"},{"location":"user-guide/troubleshooting/#log-analysis","title":"Log Analysis","text":"<pre><code># Capture detailed logs\nmake dev 2&gt;&amp;1 | tee debug.log\n\n# Analyze patterns\ngrep -i error debug.log\ngrep -i failed debug.log\ngrep -i timeout debug.log\n\n# Monitor in real-time\ntail -f debug.log | grep -E \"(error|failed|timeout)\"\n</code></pre>"},{"location":"user-guide/troubleshooting/#network-debugging","title":"Network Debugging","text":"<pre><code># Monitor network traffic\nsudo tcpdump -i any host api.openai.com\n\n# Check DNS resolution\nnslookup api.openai.com\n\n# Test with verbose curl\ncurl -v https://api.openai.com/v1/models \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\"\n</code></pre>"},{"location":"user-guide/troubleshooting/#error-codes-reference","title":"Error Codes Reference","text":""},{"location":"user-guide/troubleshooting/#application-error-codes","title":"Application Error Codes","text":"<ul> <li><code>OpenAI API key not configured</code>: Missing or empty API key in config</li> <li><code>System tray failed</code>: Desktop environment doesn't support system tray</li> <li><code>Recording failed</code>: Audio system not available or misconfigured</li> <li><code>Transcription failed</code>: API error or network issue</li> <li><code>Clipboard failed</code>: Clipboard tools not installed or display issue</li> </ul>"},{"location":"user-guide/troubleshooting/#exit-codes","title":"Exit Codes","text":"<ul> <li><code>0</code>: Normal exit</li> <li><code>1</code>: Configuration error or initialization failure</li> <li><code>130</code>: Interrupted by user (Ctrl+C)</li> </ul>"},{"location":"user-guide/troubleshooting/#log-message-patterns","title":"Log Message Patterns","text":"<pre><code># Normal operation\n[INFO] Voice Transcriber initialized successfully\n[INFO] Starting recording...\n[INFO] Transcribing audio...\n[INFO] Text copied to clipboard successfully\n\n# Error patterns\n[ERROR] Recording failed: ...\n[ERROR] Transcription failed: ...\n[ERROR] Clipboard failed: ...\n[ERROR] System tray failed: ...\n</code></pre>"},{"location":"user-guide/troubleshooting/#getting-help","title":"Getting Help","text":""},{"location":"user-guide/troubleshooting/#before-reporting-issues","title":"Before Reporting Issues","text":"<ol> <li>Check Known Issues: Review this troubleshooting guide</li> <li>Test Basic Functionality: Run <code>make test</code> and <code>make check-deps</code></li> <li>Gather System Information:    <pre><code># System info\nuname -a\nlsb_release -a\necho $XDG_CURRENT_DESKTOP\n\n# Audio info\narecord -l\npactl info  # If using PulseAudio\n\n# Application info\nbun --version\nmake get-version\n</code></pre></li> </ol>"},{"location":"user-guide/troubleshooting/#issue-reporting-template","title":"Issue Reporting Template","text":"<p>When reporting issues, include:</p> <p><pre><code>**Environment:**\n- OS: Ubuntu 22.04\n- Desktop: GNOME 42\n- Bun version: 1.2.0\n- Audio system: PulseAudio\n\n**Issue:**\nBrief description of the problem\n\n**Steps to Reproduce:**\n1. Step one\n2. Step two\n3. Step three\n\n**Expected Behavior:**\nWhat should happen\n\n**Actual Behavior:**\nWhat actually happens\n\n**Logs:**\n</code></pre> Paste relevant log output here <pre><code>**System Check:**\n</code></pre> Output of: make check-deps <pre><code>\n</code></pre></p>"},{"location":"user-guide/troubleshooting/#community-resources","title":"Community Resources","text":"<ul> <li>GitHub Issues: https://github.com/Nouuu/voice-transcriber/issues</li> <li>Documentation: https://github.com/Nouuu/voice-transcriber#readme</li> <li>npm Package: https://www.npmjs.com/package/voice-transcriber</li> </ul> <p>This troubleshooting guide covers the most common issues encountered with Voice Transcriber. Most problems can be resolved by following these step-by-step solutions.</p>"}]}