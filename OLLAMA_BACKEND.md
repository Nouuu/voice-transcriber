# Ollama LLM Backend Feature

**Feature**: Local LLM Backend for Text Formatting using Ollama
**Created**: 2025-10-21
**Status**: ğŸ“‹ Planning / Future Enhancement
**Priority**: Medium
**Estimated Effort**: 4-5 hours

---

## ğŸ¯ Motivation

Actuellement, le formatage de texte dÃ©pend exclusivement d'OpenAI GPT-3.5, ce qui implique :
- CoÃ»ts rÃ©currents (~$0.0005 par transcription)
- DÃ©pendance Ã  une connexion Internet
- Envoi des donnÃ©es dans le cloud

Cette feature ajoute **Ollama** comme backend alternatif pour permettre un **formatage 100% local**, gratuit et privÃ©.

---

## ğŸ¤– Pourquoi Ollama ?

**Ollama** est l'outil de rÃ©fÃ©rence pour exÃ©cuter des LLM localement :
- âœ… Simple Ã  installer (`curl https://ollama.ai/install.sh | sh`)
- âœ… API compatible OpenAI (`/v1/chat/completions`)
- âœ… Gestion automatique des modÃ¨les
- âœ… Optimisations CPU/GPU intÃ©grÃ©es
- âœ… Large communautÃ© et support

**Speaches** reste dÃ©diÃ© Ã  Whisper (transcription), ce qui simplifie l'architecture.

---

## âœ¨ Features ProposÃ©es

### 1. Backend Selector ğŸ”„

**Description** : Switch rapide entre OpenAI et Ollama pour le formatage

**Menu Item** :
```
ğŸ¤ Voice Transcriber
â”œâ”€â”€ ğŸ™ï¸ Start Recording
â”œâ”€â”€ â¹ï¸ Stop Recording
â”œâ”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”œâ”€â”€ âœï¸ Formatter: ON
â”œâ”€â”€â”€â”€ ğŸ¤– Backend  â† NOUVEAU
â”‚     â”œâ”€â”€ âœ“ OpenAI GPT
â”‚     â””â”€â”€ â˜ Ollama LLM (if available)
â”œâ”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”œâ”€â”€ âš™ï¸ Open Config
â”œâ”€â”€ ğŸ”„ Reload Config
â””â”€â”€ âŒ Exit
```

**Comportement** :
- DÃ©tection automatique si Ollama est disponible
- Si Ollama indisponible, griser l'option
- Changement instantanÃ© du backend de formatage
- Fallback sur OpenAI si Ollama fail

### 2. Toggle Benchmark Mode ğŸ“Š

**Description** : Activer le mode benchmark pour comparer les backends cÃ´te Ã  cÃ´te

**Menu Item** :
```
ğŸ¤ Voice Transcriber
â”œâ”€â”€ ğŸ™ï¸ Start Recording
â”œâ”€â”€ â¹ï¸ Stop Recording
â”œâ”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”œâ”€â”€ âœï¸ Formatter: ON
â”œâ”€â”€â”€â”€ ğŸ¤– Backend: OpenAI GPT
â”œâ”€â”€â”€â”€ ğŸ“Š Benchmark Mode: OFF  â† NOUVEAU (checkable)
â”œâ”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”œâ”€â”€ âš™ï¸ Open Config
â”œâ”€â”€ ğŸ”„ Reload Config
â””â”€â”€ âŒ Exit
```

**Comportement** :
- Quand activÃ© : exÃ©cute TOUS les backends disponibles en parallÃ¨le
- Copie le rÃ©sultat du backend principal dans le clipboard
- Affiche une notification comparative avec :
  - Temps de rÃ©ponse de chaque backend
  - Taille des rÃ©sultats (caractÃ¨res)
  - Backend le plus rapide
- Log dÃ©taillÃ© dans la console pour analyse

**Exemple de notification** :
```
ğŸ Benchmark Results (3.2s total):
âœ… OpenAI GPT: 1.8s - 145 chars
âœ… Ollama (tinyllama): 2.1s - 142 chars
â­ Fastest: OpenAI GPT
ğŸ“‹ Copied: OpenAI GPT result
```

**Avantages** :
- Compare qualitÃ© et vitesse des backends
- Aide Ã  choisir le meilleur backend pour son usage
- Utile pour tester de nouveaux modÃ¨les Ollama
- DonnÃ©es objectives pour optimiser la config

---

## ğŸ—ï¸ Architecture Technique

### Architecture Globale

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Audio Input    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Speaches (Whisper)      â”‚  â† Transcription uniquement
    â”‚ faster-whisper-base     â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Raw Transcription
         â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                 â”‚                  â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ OpenAI    â”‚   â”‚   Ollama     â”‚   â”‚ Pas de       â”‚
    â”‚ GPT-3.5   â”‚   â”‚  TinyLlama   â”‚   â”‚ formatage    â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                 â”‚                  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                      Formatted Text
                           â–¼
                       Clipboard
```

**SÃ©paration claire** :
- **Speaches** = Backend de transcription Whisper (dÃ©jÃ  implÃ©mentÃ©)
- **OpenAI** = Backend de formatage LLM (dÃ©jÃ  implÃ©mentÃ©)
- **Ollama** = Backend de formatage LLM local (nouveau)

### Ã‰tat en MÃ©moire (Runtime State)

```typescript
interface RuntimeState {
  formatterBackend: "openai" | "ollama";  // Backend de FORMATAGE uniquement
  benchmarkMode: boolean;                  // Toggle benchmark mode
}
```

### Configuration Ã©tendue

```json
{
  "language": "en",
  "formatterEnabled": true,
  "transcription": {
    "backend": "speaches",  // Pour Whisper (transcription)
    "speaches": {
      "url": "http://localhost:8000/v1",
      "apiKey": "none",
      "model": "Systran/faster-whisper-base"
    }
  },
  "formatting": {
    "backend": "openai",  // â† NOUVEAU : "openai" ou "ollama"
    "openai": {
      "apiKey": "sk-...",
      "model": "gpt-3.5-turbo"
    },
    "ollama": {
      "url": "http://localhost:11434/v1",  // Port standard Ollama
      "model": "tinyllama"
    },
    "formattingPrompt": "Fix grammar and punctuation. Keep original style."
  }
}
```

---

## ğŸ¤– ModÃ¨les LLM RecommandÃ©s pour CPU

### 1. **TinyLlama-1.1B** â­ (RecommandÃ© pour dÃ©marrer)
- **Taille** : 1.1B paramÃ¨tres (~1.4GB)
- **Performance CPU** : TrÃ¨s rapide (inference <1s sur CPU moderne)
- **Usage mÃ©moire** : ~2GB RAM
- **QualitÃ©** : Suffisant pour formatage basique (grammaire, ponctuation)
- **Installation** : `ollama pull tinyllama`

### 2. **Phi-2 (2.7B)** (Alternative plus puissante)
- **Taille** : 2.7B paramÃ¨tres (~1.7GB)
- **Performance CPU** : Acceptable (inference 2-3s)
- **Usage mÃ©moire** : ~4GB RAM
- **QualitÃ©** : Meilleure comprÃ©hension du contexte
- **Installation** : `ollama pull phi`

### 3. **Gemma-2B** (Alternative Google performante)
- **Taille** : 2B paramÃ¨tres (~1.7GB)
- **Performance CPU** : Rapide (inference ~1.5s)
- **Usage mÃ©moire** : ~3GB RAM
- **QualitÃ©** : Bon Ã©quilibre qualitÃ©/vitesse
- **Installation** : `ollama pull gemma:2b`

### Comparaison vs OpenAI GPT

| Aspect | TinyLlama (CPU) | Phi-2 (CPU) | OpenAI GPT-3.5 |
|--------|-----------------|-------------|----------------|
| **Latence** | ~0.5-1s | ~2-3s | ~1-2s |
| **CoÃ»t** | Gratuit | Gratuit | ~$0.0005/req |
| **QualitÃ©** | Basique | Bonne | Excellente |
| **Privacy** | 100% local | 100% local | Cloud |
| **RAM requise** | 2GB | 4GB | N/A |
| **Setup** | 1 commande | 1 commande | API key |

---

## ğŸ“Š Plan d'ImplÃ©mentation

### Phase 1: Ollama Integration - 2h
- [ ] Installer et tester Ollama avec TinyLlama
- [ ] Ajouter FormatterService.checkOllamaAvailability()
- [ ] Ã‰tendre FormatterService pour supporter backend "ollama"
- [ ] ImplÃ©menter formatWithOllama() avec API compatible OpenAI
- [ ] Tests unitaires (3-4 tests)

### Phase 2: Backend Selector Menu - 1h
- [ ] Ajouter formatterBackend dans RuntimeState
- [ ] Ajouter submenu "Backend" (OpenAI/Ollama)
- [ ] ImplÃ©menter handleBackendChange()
- [ ] Griser l'option Ollama si indisponible
- [ ] Tests unitaires (2-3 tests)

### Phase 3: Benchmark Mode - 2h
- [ ] Ajouter benchmarkMode dans RuntimeState
- [ ] Ajouter menu item checkbox "Benchmark Mode ON/OFF"
- [ ] ImplÃ©menter exÃ©cution parallÃ¨le de tous les backends
- [ ] CrÃ©er notification comparative avec timings et stats
- [ ] Logger dÃ©taillÃ© des rÃ©sultats pour analyse
- [ ] Tests unitaires (3-4 tests)

### Phase 4: Documentation & Polish - 1h
- [ ] Documentation Ollama setup dÃ©taillÃ©
- [ ] Documentation Benchmark Mode (utilisation, interprÃ©tation des rÃ©sultats)
- [ ] Guide de troubleshooting (Ollama connection, modÃ¨les)
- [ ] Exemples de configuration pour diffÃ©rents modÃ¨les
- [ ] Messages d'erreur clairs si Ollama indisponible

**Total estimÃ©** : 6 heures

---

## ğŸ¯ BÃ©nÃ©fices Utilisateur

### Ollama Local Backend
- âœ… **Zero cost** pour formatage (aprÃ¨s setup)
- âœ… **100% privacy** - tout en local
- âœ… **Pas de latence rÃ©seau** - plus rapide sur bon CPU
- âœ… **Pas de limites** - formatage illimitÃ©
- âœ… **Setup en 2 commandes** - Installation simple
- âœ… **API compatible OpenAI** - IntÃ©gration facile
- âœ… **Fallback automatique** si Ollama indisponible

### Benchmark Mode
- âœ… **Comparaison objective** - Mesures prÃ©cises de performance
- âœ… **Aide Ã  la dÃ©cision** - Choisir le meilleur backend pour son usage
- âœ… **Test de modÃ¨les** - Ã‰valuer rapidement de nouveaux modÃ¨les Ollama
- âœ… **Optimisation** - DonnÃ©es concrÃ¨tes pour ajuster sa config
- âœ… **Transparence** - Voir la diffÃ©rence de qualitÃ©/vitesse entre backends

---

## ğŸš§ Limitations & ConsidÃ©rations

### Limitations Techniques
- Ollama nÃ©cessite 2-6GB RAM selon le modÃ¨le
- QualitÃ© formatage Ollama < OpenAI GPT (mais acceptable)
- Performance CPU-dÃ©pendante (peut Ãªtre lente sur vieux CPU)
- NÃ©cessite installation sÃ©parÃ©e d'Ollama

### DÃ©cisions de Design
- **Speaches = Whisper uniquement** : SÃ©paration claire transcription/formatage
- **Ollama = Formatage local** : Outil dÃ©diÃ© pour LLM locaux
- **Benchmark en option** : Pas activÃ© par dÃ©faut (overhead)

---

## ğŸ“š Documentation Utilisateur

### Setup Rapide Ollama

```bash
# 1. Installer Ollama (Linux)
curl https://ollama.ai/install.sh | sh

# 2. Installer Ollama (macOS)
brew install ollama

# 3. DÃ©marrer le service
ollama serve  # Ou dÃ©marre automatiquement

# 4. TÃ©lÃ©charger un modÃ¨le
ollama pull tinyllama    # Rapide, 2GB RAM
ollama pull phi          # Plus puissant, 4GB RAM
ollama pull gemma:2b     # Alternative Google

# 5. VÃ©rifier les modÃ¨les installÃ©s
ollama list

# 6. Tester un modÃ¨le
ollama run tinyllama "Fix grammar: i gone to store yesterday"
```

### Configuration Voice Transcriber

```json
{
  "formatting": {
    "backend": "ollama",
    "ollama": {
      "url": "http://localhost:11434/v1",
      "model": "tinyllama"
    }
  }
}
```

### Utilisation

**Switch Backend** :
1. Install Ollama and pull a model (see above)
2. Right-click system tray â†’ "Backend" â†’ "Ollama LLM"
3. Next transcription uses local formatting

**Compare Backends** :
1. Install Ollama with at least one model
2. Right-click â†’ "Benchmark Mode: ON"
3. Record and transcribe as usual
4. See comparative notification with timings
5. Check console logs for detailed analysis

---

## âœ… Checklist de Validation

Avant de dÃ©marrer l'implÃ©mentation :

- [ ] Installer et tester Ollama avec TinyLlama (qualitÃ© formatage acceptable ?)
- [ ] VÃ©rifier que l'API Ollama `/v1/chat/completions` fonctionne correctement
- [ ] Tester la latence Ollama vs OpenAI sur diffÃ©rents CPU
- [ ] Confirmer que le fallback OpenAI fonctionne si Ollama Ã©choue
- [ ] Valider le format de notification benchmark (lisible, utile)

---

**Status** : âœ… **PrÃªt pour implÃ©mentation**
**DÃ©pendances** : Ollama installÃ© sur la machine (optionnel)
**Related** : Voir `QUICK_ACTIONS_MENU.md` pour le menu interactif

